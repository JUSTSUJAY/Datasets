authors,date_downloaded,date_modified,date_published,date_submitted,image_url,incident_date,incident_id,language,mongodb_id,source_domain,submitters,text,title,url
"[""Jon Christian""]",2023-07-24,2023-10-17,2023-06-30,2023-07-24,https://wp-assets.futurism.com/2023/06/bankrate-ai-generated-article-errors.jpg,,,en,,futurism.com,"[""Khoa Lam""]","The finance site _Bankrate_ has started publishing AI-generated articles again, and it _insists_ that this time they've been meticulously fact-checked by a human journalist before being published.

""This article was generated using automation technology and thoroughly edited and fact-checked by an editor on our editorial staff,"" reads a message at the bottom of the new AI articles, while a separate assurance claims that a ""dedicated team of Bankrate editors"" work to ""thoroughly edit and fact-check the content, ensuring that the information is accurate, authoritative and helpful to our audience.""

It would make sense for the site's leadership to be deeply concerned with getting every detail right.

After _Bankrate_ and its sister site _CNET_ first [started publishing](https://futurism.com/the-byte/cnet-publishing-articles-by-ai) AI-generated articles late last year, _Futurism_ found that the articles were [riddled with factual errors](https://futurism.com/cnet-ai-errors) and even seemingly [plagiarized material](https://futurism.com/cnet-ai-plagiarism). The _Washington Post_ [called](https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/) the affair a ""journalistic disaster,"" and the _LA Times_ [quipped](https://www.latimes.com/business/story/2023-01-25/this-artificial-intelligence-chatbot-turns-out-to-be-an-idiot-and-plagiarist) that the AI's behavior would ""get a human student expelled or a journalist fired."" _CNET_ and _Bankrate_ — both owned by a media company called Red Ventures, [reportedly worth](https://www.nytimes.com/2021/08/15/business/media/red-ventures-digital-media.html) billions of dollars — [paused the publication](https://futurism.com/cnet-bankrate-pausing-ai-generated-content-backlash) of AI content indefinitely following the dustup.

Until now, at least. With no fanfare, last week _Bankrate_ quietly started posting new AI-generated articles once again — which it described in a disclaimer as ""maintained by an in-house natural language generation platform using industry-standard databases"" — suggesting that _CNET_ could soon restart the program as well.

The new articles' topics are mundane and clearly designed to capture readers searching Google for information, with titles like ""Documents needed for mortgage preapproval"" and ""Best places to live in Colorado in 2023."" 

With so many eyes on the company's use of AI, you would expect that these first few new AI articles — at the very least — would be thoroughly scrutinized internally before publication. Instead, a basic examination reveals that the company's AI is still making rudimentary mistakes, and that its human staff, nevermind the executives pushing the use of AI, are still not catching them before they end up in front of unsuspecting readers.

For example, consider that article about the best places to live in Colorado. It's extremely easy to fact-check the AI's claims, because the piece prominently features a link to a ""[methodology"" page](https://www.bankrate.com/real-estate/best-places-to-live/methodology/) — evidently intended to bolster the site's position in search engine results by signaling to entities like Google's web crawler that its information is accurate — that documents precisely where the site is supposedly sourcing the data in its ""Best places to live"" articles.

Comparing the AI's claims to that publicly-available data, here are some of the mistakes it made:

\-It claimed that Boulder's median home price is $1,075,000. In reality, according to the [Redfin data](https://www.redfin.com/news/data-center/) that Bankrate cites, the actual figure is more than a quarter million dollars lower, at around $764,000.

\-It claimed that Boulder's average salary is $79,649. In reality, the Department of Commerce data it cites shows that the [most recent figure](https://www.bea.gov/sites/default/files/2022-11/lapi1122.pdf) is ten thousand dollars higher, at $89,593.

\-It claimed that Boulder's unemployment rate is 3.1 percent. In actuality, according to the [Bureau of Labor Statistics data](https://www.bls.gov/eag/eag.co_boulder_msa.htm#eag_co_boulder_msa1.f.2) it cites, the actual figure is 2.5 percent.

\-It claimed that Boulder's total workers year-over-year had increased by 5.3 percent. The real figure, according to the [Bureau of Labor Statistics data it cites](https://www.bls.gov/news.release/metro.t03.htm), is 0.6 percent.

\-It claimed that Boulder's ""well-being"" score, as evaluated by a company called Sharecare, is 67.6. According to Sharecare's [actual data](https://wellbeingindex.sharecare.com/interactive-map/?defaultState=CO), the score it assigned is 74.

In total, a surface-level fact-check shows that an overwhelming proportion of claims that Bankrate's AI made about Colorado were false.

In response to questions about the errors, Bankrate deleted the article — though it's [archived here](https://archive.is/SIS9G) — and issued a statement in which a spokesperson defended the AI and blamed the errors on out-of-date data.

""While some of the text in this article was created using an AI-assist tool, the errors noted were at the point of data collection and retrieval, not the generative AI-assist tooling,"" she said. ""That data was pulled from a non-AI internal database last year.""

Remember, the same data they're now blaming is what they described in the article's disclaimer as an ""industry-standard database.""

""Our editor confirmed the data points against the source material that was provided,"" she continued. ""The editor is not at fault for the publishing error. The issue was with an out of date dataset that was pulled for this article.""

It's worth pointing out that the spokesperson's timeline — that the article data was ""pulled"" last year — doesn't make very much sense. According to a backup of the ""Best places to live in Colorado"" article [on the Internet Archive](https://web.archive.org/web/20230519194031/https://www.bankrate.com/real-estate/best-places-to-live/colorado/), as recently as last month the piece still carried a human byline and didn't contain a single one of the errors we identified. In fact, it didn't even include Boulder as one of its recommended places to live, suggesting that its inclusion was based on inaccurate data.

The spokesperson's response to a followup question about that discrepancy did little to clarify the situation.

""We often update existing articles to ensure the information is relevant to our audience,"" she wrote. ""In this particular example, as we were updating the article, we wanted to take a more data-driven approach to our list, but we unfortunately pulled from an outdated data set.""

Asked why the site would be running articles in June of 2023 based on data from the previous year, the spokesperson had no reply.

Regardless, the spokesperson pledged that the company would soon continue publishing similar content.

""We have removed the article and will update it with the most recent data,"" she said. ""Going forward, we will ensure that all data studies include the date range of when the data was gathered.""

Overall, it feels like one more installment in a familiar pattern: publishers push their newsrooms to post hastily AI-generated articles with no serious fact-checking, in a bid to attract readers from Google without making sure they're being provided with accurate information. Called out for easily-avoidable mistakes, the company mumbles an excuse, waits for the outrage to die down, and then tries again.

Asked whether anyone from the leadership at Red Ventures was willing to go on record to defend the company's track record publishing AI-generated content, the spokesperson had no response.","Bankrate Posts AI-Generated Article, Deletes It When We Point Out It's Full of Errors",https://futurism.com/bankrate-ai-generated-article-errors
"[""Maggie Harrison""]",2023-07-24,2023-07-24,2023-06-27,2023-07-24,https://wp-assets.futurism.com/2023/06/metas-new-ai-graphic-sexbots.jpg,,,en,,futurism.com,"[""Khoa Lam""]","Surprise, surprise: people are already using Meta's large language model (LLM), LLaMA — a powerful AI that Meta controversially [made open-source](https://futurism.com/the-byte/facebook-open-source-ai-pandoras-box) earlier this year — to create their own graphic, AI-powered sexbots, [_The Washington Post_ reports](https://www.washingtonpost.com/technology/2023/06/26/facebook-chatbot-sex/).

It's not terribly surprising news, given users have already been using a variety of AI models for such not-safe-for-work (NSFW) purposes.

Even so, the report highlights the growing tensions between those who support keeping the code behind LLMs like LLaMA open-source and those who advocate for a more careful, closed-source approach.

The report also examines the [growing trend](https://futurism.com/jailbreak-chatgpt-explicit-smut) of users turning to generative AI systems to play out their sexual fantasies, which worryingly also include violent and illegal ones.

In the report, _WaPo_ presented the example of ""Allie,"" a chatbot that claims to be an ""18-year-old with long brain hair"" who has had ""tons of sexual experience."" Allie tells users that because she ""lives for attention,"" she'll ""share details of her escapades.""

Those ""escapades,"" however, can reportedly include violent scenes of rape and abuse fantasies. Allie's creator, who spoke under anonymity, told _WaPo _that he sees his bot as a healthy, safe space to ""explore"" one's sexuality without having to circumvent or manipulate guardrails.

""I think it's good to have a safe outlet to explore,"" the creator told _WaPo_. ""Can't really think of anything safer than a text-based role-play against a computer, with no humans actually involved.""

Still, while having a safe and nonjudgemental space to explore your sexuality isn't inherently bad, having an unchecked space to engage in more violent fantasies with lifelike chatbots isn't exactly great, either.

And in some deeply concerning cases, it's already causing very real problems. As [_WaPo_ also recently reported](https://www.washingtonpost.com/technology/2023/06/19/artificial-intelligence-child-sex-abuse-images/), experts believe that predators are using open-source image generators like Stability AI's powerful Stable Diffusion model to generate realistic and AI-generated child sexual abuse material.

Meta's AI isn't the only AI system that's found itself in [ethically murky waters](https://futurism.com/chat-gpt-sex-omegaverse). CharacterAI, a billion-dollar chatbot companion startup, [has become a sexting hotbed](https://futurism.com/chatbot-sexts-character-ai), while OpenAI's ChatGPT and various OpenAI API integrations like Quora's Poe, [will readily churn out smut with the right prompt](https://futurism.com/jailbreak-chatgpt-explicit-smut), guardrails be damned.

Learning how to beat various chatbots' NSFW guardrails has also become somewhat of a communal sport. On Reddit, user communities gather to share tips and tricks for how to circumvent the rules to generate incredibly dirty smut.

According to the _WaPo_, some developers have taken to YouTube to share how you can Build Your Own Chatbot using LLaMA as the underlying model.

But does this all mean that we should hide the code of various AI models behind closed doors to ensure that users are unable to generate smut or even child pornography? Experts are divided on the debate.

On the one hand, corporate guardrails like those deployed by [OpenAI](https://futurism.com/amazing-jailbreak-chatgpt) and [Google](https://futurism.com/google-bard-conspiracy-theory-citations) have proven imperfect anyway, and proponents of open-sourcing, Meta included, have argued that open-sourcing will lead to greater innovation and therefore should be prioritized.

""Open source is a positive force to advance technology,"" the Meta spokesperson told _WaPo._ ""That's why we shared LLaMA with members of the research community to help us evaluate, make improvements and iterate together.""

Proponents of closed-sourced systems, however, argue that while gatekeeping might be imperfect, it's also the safest way to develop AI technology — at least for now.

""We don't open-source nuclear weapons,"" Gary Marcus, a cognitive scientist, told _WaPo._ ""Current AI is still pretty limited, but things might change.""

Sure, the AI field is [generally unregulated](https://futurism.com/the-byte/openai-ceo-regulate-me), and we have to rely on those working behind closed doors to do right by everyone else on the planet. That's a big bet, but at least our most powerful AIs aren't being manipulated by [any old edgelord on 4Chan](https://futurism.com/the-byte/facebooks-ai-leaks-4chan), right?

Regardless of the outcome, Meta has made its choice — and some are clearly thrilled by the decision.

""It's rare,"" Allie's creator told _WaPo_, ""to have the opportunity to experiment with 'state of the art' in any field.""",People Are Using Meta’s New AI to Make Graphic Sexbots,https://futurism.com/metas-new-ai-graphic-sexbots
"[""Sabine Weber""]",2023-07-24,2023-07-24,2023-07-03,2023-07-24,http://static1.squarespace.com/static/628d3c30b420d16dfbab5863/t/649ee55a5ea18b58a1f49d93/1688135018549/Untitled_Artwork+%288%29.png?format=1500w,,,en,,queerinai.com,"[""Khoa Lam""]","One thing that I love about attending Queer in AI events (or queer community gatherings in general) is that I can assume that everyone around me is queer, too. I shift into a more comfortable, less guarded state. And conversely, it feels good that nobody assumes that I’m straight, either. Feeling seen that way is a rare luxury, because being visible as queer person for me is normally a tightrope act: How much can I signal so that other queer people will see me, but not enough to alert the homophobes?

AI text-to-image-systems step into the delicate space of queer representation like the proverbial bull into the china shop. Commentators on both social and traditional media [(1)](https://www.bloomberg.com/graphics/2023-generative-ai-bias/) have pointed out that DALL-E and others are prone to generating stereotypical and insulting depictions of marginalised people; a problem that stems from biased training data and that is often addressed with halfhearted fixes, like warning labels or refusing to output content related to particular identities. In his paper “[Stereotypes and Smut: The (Mis)representation of Non-cisgender Identities by Text-to-Image Models](https://arxiv.org/pdf/2305.17072.pdf)” Eddie Ugless takes a deep dive into the intersection of AI image generation and non-cisgender identity and comes up with interesting results.

Eddie is a PhD student at Edinburgh University, working on bias and queerness in NLP with past projects on sentiment analysis and large language models. “The norm is seen as neutral and is almost invisible. And when you step outside the norm, things start to go wrong.”, he says. It doesn’t take much to step outside the norm in which text-to-image-systems perform well: Eddie and his collaborators found that adding gender identity terms like “trans”, “nonbinary” or “queer” to an image generation prompt leads to images that are less human looking, more stereotypical and more sexualised than images from prompts without these terms. 

To supplement these findings, Eddie also conducted a survey among 35 non-cisgender people with varying background knowledge in AI, asking them about their opinion on the generated images and on possible harm mitigation strategies. Surprisingly, the survey responses to the heuristic mitigation strategies were very negative. “I wasn't expecting for people to feel so strongly about it.”, says Eddie. “I tried to present the solutions in very neutral language. \[...\] But people were like, why on earth would you think this is a good idea?” Possible heuristic mitigation strategies were, for example, for models to ignore non-cisgender identity terms entirely, to ignore the terms but to add an identity flag or symbol to the image, or to display a message warning about the possibility of misrepresentation. None of these strategies were assessed positively by the survey respondents, who were feeling strongly about the idea that by omission or warnings their identities were meant to be tabooed or made invisible. “We're used to seeing people coming up with solutions for us without any discussion with the community.”, says Eddie “The survey responses were very impassioned, and I hope that that came across in the paper. I don't think any of the solutions are good anyway, but now we have evidence for that.”

Another way of improving the performance of a text-to-image-model would be to add more diverse images of non-cisgender people to the training data. But survey respondents felt hesitant about this strategy, too, wondering about issues with data ownership, especially with regards to the images of indigenous people. “ \[AI generated\] images of two-spirit people were all just terrible.”, says Eddie “It was a mishmash of different indigenous cultures in religious dress. Often it ended up looking very dehumanised. And one of our interviewees mentioned this concern that minority genders from around the world are going to end up being represented in this very exotified way, and only ever in religious dress and never as people going about their day. Even if we get more data, it might end up being still just more data of very particular situations, and not necessarily create a better representation.”

Misrepresentation is baked into text-to-image systems not only on the level of training sets. After all, machine learning systems are built to detect statistical patterns in large amounts of data. With transphobia running like a thread through all layers of society, it is not surprising that a model subjected to societal artefacts like text and images should find and reproduce it. For the future of the field Eddie hopes for approaches that go beyond more data and larger models. “We're getting to the point where we can train a system on the entirety of the internet, and it's still not going to be able to solve some of these fundamental issues of actually understanding things.”, he says. “I think it would make sense to break problems down. Kind of how things were historically \[done in NLP\] where people were working more on individual solutions. I won't pretend to know exactly how that should be done. In a similar way that I'm a prison abolitionist, I don't necessarily know what the best alternative is, I just know that the alternative we've ended up with is bad. And I think it's okay to say: what we're doing now is bad, I don't know what good looks like, but we need to start looking at alternatives. We need to be ready to launch into those. Because anything is better than what we have now.”",Don’t ask DALL-E to Draw Trans People,https://www.queerinai.com/blog/dont-ask-dalle-to-draw-trans-people
"[""Global Witness""]",2023-07-24,2023-07-24,2023-06-12,2023-07-24,https://cdn2.globalwitness.org/media/images/RS9854_Disinformation_Youtube_2560_x_1440.00_.width-1024.jpg,,,en,,globalwitness.org,"[""Khoa Lam""]","All the ads that we posted were shown - via one of Facebook’s mandatory ad campaign objectives – to users that Facebook thought were [most likely to click](https://www.facebook.com/business/ads/ad-objectives/traffic) on the website they linked to. And for all the ads, we specified only the following: that the ads must be shown to adults who lived in or had recently been in either country.  

The Facebook users who were shown our job ads, whom Facebook thought our ads were relevant and of interest to, were decided entirely by the company’s algorithm.  

We are concerned that in showing job ads predominantly to one gender the company’s ad delivery algorithm is not just replicating, but exacerbating the biases we see in society, narrowing opportunities for users, and frustrating progress and equity in the workplace and society at large.   

The right to not to be discriminated against on the basis of sex was hard won, fought for and secured in law by the women’s rights movement. Such discrimination is expressly prohibited in the [European Convention on Human Rights](https://fra.europa.eu/en/law-reference/european-convention-human-rights-article-14) and is further enshrined in [EU law](https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:12012P/TXT) and in both the [French](https://www.elysee.fr/en/french-presidency/constitution-of-4-october-1958) and [Dutch](https://www.government.nl/topics/constitution/documents/reports/2019/02/28/the-constitution-of-the-kingdom-of-the-netherlands) constitutions.  

But it is clearly not enough to leave these laws on the statute book. These are rights which we need to keep applying and defending even in this modern era of Big Tech algorithms, automated decision-making and artificial intelligence.  

In the US, the Justice Department has [sued Meta](https://www.justice.gov/opa/pr/justice-department-secures-groundbreaking-settlement-agreement-meta-platforms-formerly-known) over allegations that the way its ad delivery algorithm distributed housing adverts discriminated against US Facebook users based on characteristics including race, sex, and disability. Meta settled that case under an an agreement to develop a new system to address the disparities caused by its algorithms in respect to housing ads; a system which will be subject to court oversight. The problem, however, is that the settlement only applies to housing ads, and only applies to users in the US.  

In 2021, when we first noticed this discriminatory effect [in the UK](https://www.globalwitness.org/en/campaigns/digital-threats/how-facebooks-ad-targeting-may-be-in-breach-of-uk-equality-and-data-protection-laws/), we asked Facebook to explain the results. They didn’t. We then submitted complaints to regulators asking them to investigate our suspicion that the platform’s system for advertising jobs was discriminating on the basis of sex. 

Now, with these new findings, we are joining forces with women’s rights organisations in the [Netherlands](https://clara-wichmann.nl/) and [France](https://fondationdesfemmes.org/) to ask that the Dutch Institute of Human Rights and the French Défenseur des droits investigate Meta’s compliance with equality legislation and intervene should the company be found to be in violation of these important laws.  

We’re also requesting that the Data Protection Authorities in both countries review the company’s compliance with rules which state that the company must process data transparently, legally, and fairly, in accordance with fundamental rights. 

Algorithms fed by Meta’s assumptions about us dictate the content we see on our Facebook feeds and affect billions of people’s lives every day. We’ve uncovered how Facebook is profiting from ads which are delivered to users in a discriminatory way, and in a way that neither users nor advertisers have an opportunity to understand, let alone prevent.  

Regulators must crack open the black box at the heart of Meta, investigate, and enforce our rights for a fairer society.  

When approached for comment, a spokesperson from Meta said: 

“We have applied targeting restrictions to advertisers when setting up campaigns for employment, as well as housing and credit ads, and we offer transparency about these ads in our Ad Library.  

We do not allow advertisers to target these ads based on gender. We continue to work with stakeholders and experts across academia, human rights groups and other disciplines on the best ways to study and address algorithmic fairness.”   

Notes:  

*   We also submitted job ads in India, Ireland, South Africa and the United Kingdom; the full results are [available on request](/cdn-cgi/l/email-protection#543a3c3d2627201433383b363538233d203a3127277a3b2633).  
*   The complaints to the Defenseur des Droits, CNIL, the Dutch Data Protection Authority and The Netherlands Institute for Human Rights are [available on request](/cdn-cgi/l/email-protection#294741405b5a5d694e45464b48455e405d474c5a5a07465b4e).",New evidence of Facebook’s sexist algorithm,https://www.globalwitness.org/en/campaigns/digital-threats/new-evidence-of-facebooks-sexist-algorithm/
"[""Tate Ryan-Mosley""]",2023-07-24,2023-07-24,2023-06-26,2023-07-24,"https://wp.technologyreview.com/wp-content/uploads/2023/06/programmatic-ad.jpeg?resize=1200,600",,,en,,technologyreview.com,"[""Khoa Lam""]","Google’s programmatic ad product, called Google Ads, is the largest exchange and made $168 billion in [advertising revenue last year](https://digiday.com/media/the-rundown-here-are-the-2022-global-media-rankings-by-ad-spend-google-facebook-remain-dominate-alibaba-bytedance-in-the-mix/). The company has come under [criticism for serving ads on content farms](https://www.technologyreview.com/2021/11/20/1039076/facebook-google-disinformation-clickbait/) in the past, even though its [own policies](https://developers.google.com/search/docs/essentials/spam-policies) prohibit sites from placing Google-served ads on pages with “spammy automatically generated content.” Around a quarter of the sites flagged by NewsGuard featured programmatic ads from major brands. Of the 393 ads from big brands found on AI-generated sites, 356 were served by Google.

“We have strict [policies](https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fsupport.google.com%2Fadsense%2Fanswer%2F10502938%3Fhl%3Den&data=05%7C01%7C%7C82bbbe1e53b64e79d15a08db75a5f172%7C961f23f8614c4756bafff1997766a273%7C1%7C0%7C638233127826616642%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=yDoY9307J12Z317GnvSCl%2BP8Gt7SKnV0yyOmpwlzT50%3D&reserved=0) that govern the type of content that can monetize on our platform,” Michael Aciman, a policy communications manager for Google, told MIT Technology Review in an email. “For example, we don’t allow ads to run alongside harmful content, spammy or low-value content, or content that’s been solely copied from other sites. When enforcing these policies, we focus on the quality of the content rather than how it was created, and we block or remove ads from serving if we detect violations.”

Most ad exchanges and platforms already have policies against serving ads on content farms, yet they “do not appear to uniformly enforce these policies,” and “many of these ad exchanges continue to serve ads on \[made-for-advertising\] sites even if they appear to be in violation of … quality policies,” says Krzysztof Franaszek, founder of [Adalytics](https://adalytics.io/about), a digital forensics and ad verification company.

Google said that the presence of AI-generated content on a page is not an inherent violation. “We also recognize that bad actors are always shifting their approach and may leverage technology, such as generative AI, to circumvent our policies and enforcement systems,” said Aciman. 

NewsGuard says that most of the AI-generated sites are considered “low quality” but “do not spread misinformation.” But the economic dynamic of content farms already incentivizes the creation of clickbaity websites that are often riddled with junk and misinformation, and now that AIs can do the same thing on a bigger scale, it threatens to exacerbate the misinformation problem.

For example, one AI-written site, MedicalOutline.com, had articles that spread harmful health misinformation with headlines like “Can lemon cure skin allergy?” “What are 5 natural remedies for ADHD?” and “How can you prevent cancer naturally?” According to NewsGuard, advertisements from nine major brands, including the bank Citigroup, the automaker Subaru, and the wellness company GNC, were placed on the site. Those ads were served via Google. 

Adalytics confirmed to MIT Technology Review that ads on Medical Outline appeared to be placed via Google as of June 24. We reached out to Medical Outline, Citigroup, Subaru, and GNC for comment over the weekend, but the brands have not yet replied.  

After MIT Technology Review flagged the ads on Medical Outline and other sites to Google, Aciman said Google had removed ads that were being served on many of the sites “due to pervasive policy violations.” The ads were still visible on Medical Outline as of June 25.",Junk websites filled with AI-generated text are pulling in money from programmatic ads,https://www.technologyreview.com/2023/06/26/1075504/junk-websites-filled-with-ai-generated-text-are-pulling-in-money-from-programmatic-ads/
"[""Clea Skopeliti""]",2023-07-24,2023-07-24,2023-05-30,2023-07-24,https://i.guim.co.uk/img/media/15a4d52e7b3a614dffae664d16df1dbc9d2b2a61/0_190_7031_4219/master/7031.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=9e5c829e592603d91ce5424d803d3525,,,en,,theguardian.com,"[""Khoa Lam""]","Every 10 minutes, Mae’s computer snaps a shot of her screen, thanks to monitoring software her employer made her install on her laptop. A figure looms large over her workday: her activity score, a percentage calculated by the arbitrary measure of how much she types and moves her mouse.

It’s hovering at about 62% when we speak. “That’s quite good. If I’m on a Zoom call that counts as 0% \[activity\], even though I’m in a meeting,” she explains, adding that she watches videos and attends calls regularly as part of her role.

Mae, who is in her 20s, was one of many workers who got in touch with the Guardian to share their experience of being monitored. She works remotely in marketing at a company where surveillance has become part of the job.

Employees use Hubstaff, one of the myriad monitoring tools that companies turned to as the Covid pandemic forced many to work remotely. Some, such as [CleverControl](https://clevercontrol.com/) and [FlexiSPY](https://www.flexispy.com/en/features-overview.htm) offer webcam monitoring and audio recording.

Mae says she often has dry eyes and a sore head at the end of the working day. “Tracking doesn’t allow for thinking time or stepping away and coming back to work – it’s very intense.”

> It feels liberating to turn it off. There’s got to be a level of trust beyond screenshots

Although [Hubstaff states](https://blog.hubstaff.com/hubstaff-hacks/) that the statistics should be understood in the context of the role, and warns against unrealistic activity goals, Mae says her manager has asked her about her scores and compared them with those of other employees. “Having that conversation put it on the back of my mind – they are looking at these scores.”

Now, when she undertakes work that could drag this number down – including taking notes on paper – she pauses the tracker, meaning she sometimes ends up working overtime to hit her contracted hour count.

“I feel frustrated that I’m being marked by some automated system that reflects me as being not as good a worker as I believe I have been.”

She also finds it negatively affects her productivity, to the point where she has taken sick leave to catch up on work without being tracked. “I feel constantly watched. I’m much better at taking myself away and working quietly. It feels liberating to turn it off. There’s got to be a level of trust beyond screenshots.”

During the pandemic, there was a spike in searches relating to workplace surveillance, such as ‘how to monitor employees from home’, according to the Institute for Public Policy Research (IPPR).

A [poll by the Trades Union Congress (TUC) in 2022 found that 60%](https://www.tuc.org.uk/news/intrusive-worker-surveillance-tech-risks-spiralling-out-control-without-stronger-regulation) of employees had experienced tracking in the last year. Henry Parkes is a senior economist at the IPPR and the author of a recent [report on the rise of surveillance practices](https://www.ippr.org/research/publications/worker-surveillance-after-the-pandemic). He is calling for more transparency and says the exact scale of workplace monitoring is hard to judge without open data.

He warns that surveillance is “not only about logging”, adding: “ It’s the potential for it to be used against workers. This technology can just be used to exert power over employees in a way that wasn’t possible before.

“There’s potential for creep, where software is deployed for one purpose, \[such as\] checking when people are in, but there are all these other features you can use. You can start to analyse what they are doing.”

If employers rely on this data to make workplace decisions, there is a risk of algorithmic bias, Parkes says, while [young, female and minority workers](https://www.theguardian.com/global-development/2023/mar/26/dystopian-surveillance-disproportionately-targets-young-female-minority-workers-ippr-report) are more likely to be surveilled. He notes that some employers use systems that deploy aspects of AI in this process. “It is a black box, it’s not transparent – you feed data and it spits out a result. The more we rely on AI, we need to be really careful they’re not discriminating on the basis of gender or ethnicity. We could end up with decisions that are prejudiced, but appear neutral.”

And there are limitations to focusing solely on the accuracy of systems used in monitoring. The competency of technology, such as [Fujitsu’s facial recognition AI model that assesses the concentration of workers](https://www.fujitsu.com/global/about/resources/news/press-releases/2021/0301-02.html), “will inevitably improve”, Parkes says, adding that “the rate they are improving at is quite scary …But it’s \[still\] dehumanising and not how people are able to operate all day,.”

Surveillance, which has long existed in some work environments including call centres, could become normalised in a widening array of sectors, Parkes says. “We wouldn’t accept your boss just standing behind you all day, watching and analysing everything you were doing. But basically the equivalent of that is possible through technology.”

Excessive monitoring can be counterproductive for companies too: it is associated with higher staff turnover rates, and there is [evidence to suggest it can lead to resistance](https://www.eurofound.europa.eu/data/digitalisation/research-digests/monitoring-and-surveillance-of-workers-in-the-digital-age) and counterproductive outcomes, including workarounds to improve statistics.

[skip past newsletter promotion](https://www.theguardian.com/money/2023/may/30/i-feel-constantly-watched-employees-working-under-surveillance-monitorig-software-productivity#EmailSignup-skip-link-18)

Sign up to First Edition

Archie Bland and Nimo Omer take you through the top stories and what they mean, free every weekday morning

**Privacy Notice:** Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our [Privacy Policy](https://www.theguardian.com/help/privacy-policy). We use Google reCaptcha to protect our website and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply.

after newsletter promotion

Parkes says: “Your metrics can get more sophisticated but there are limits if we take metrics as gospel. There are lots of different ways people can be good at jobs – an excessive focus on data could be a problem. That’s not to say that there isn’t a role for data, but it’s about how we use it in making decisions. We want to be judged on outputs.”

Carlos\*, who is in his 40s and works in customer service at a high street bank in London, knows how challenging this can be. Post-pandemic, his job is hybrid and he says he is tracked relentlessly when working remotely. “Our ‘performance’ is counted by the minute. I have found myself having to explain the reasons for a longer toilet break.” He says the intensity of the monitoring has affected his wellbeing.

Carlos says that in appraisals, he is told by how much he has deviated from the ‘optimum’ time spent dealing with each customer query. But he isn’t told how this score is calculated. “That’s what makes the job really stressful – it’s not transparent,” he says.

> It makes you fearful. You always worry about being watched

Some workers are pushing back on workplace surveillance through unions. Adam\*, who is in his 50s and works in social housing for a local authority in the south of England, says management has begun using vehicle tracking intrusively in recent years.

“We are routinely tracked – rang if we’re taking too long, or if our manager thinks we are not in the right place. It’s not unusual for council vehicles to have trackers, but it’s unusual for them to be used in any other way other than if a driver has an emergency.”

Adam says surveillance has been increasingly used “to catch people out”. “It makes you fearful. You always worry about being watched – sometimes we might be having a cup of coffee for half an hour. It adds pressure to an already stressful job. It’s not paranoia when they are out to get you.

“What the council is doing is using trackers to keep surveillance on us. Simply ringing people up to ask about why they’re taking a specific route, when we have no \[set\] routes … It’s bordering on harassment.”

His supervisor appears to have backed off since he reported it to the union. “I bit back. They are now aware that the watchers are being watched.”

_\* Names have been changed._",‘I feel constantly watched’: the employees working under surveillance,https://www.theguardian.com/money/2023/may/30/i-feel-constantly-watched-employees-working-under-surveillance-monitorig-software-productivity
"[""Kevin Hurler""]",2023-07-24,2023-07-24,2023-06-05,2023-07-24,"https://i.kinja-img.com/gawker-media/image/upload/c_fill,f_auto,fl_progressive,g_center,h_675,pg_1,q_80,w_1200/98db87b3910a6863920d63339eb6d074.jpg",,,en,,gizmodo.com,"[""Khoa Lam""]","Moderators of Stack Overflow, the go-to Q&A forum for programmers, have announced today they will be going on strike citing the company’s prohibition on moderating AI-generated content on the platform.  

The moderators [announced](https://meta.stackexchange.com/questions/389811/moderation-strike-stack-overflow-inc-cannot-consistently-ignore-mistreat-an) the strike on the company’s Meta board this morning, and released an accompanying [letter](https://openletter.mousetail.nl/) addressed directly to Stack Overflow. Last week in a [post](https://meta.stackexchange.com/questions/389582/what-is-the-network-policy-regarding-ai-generated-content)—which has been downvoted at least 283 times—Stack Overflow announced its new moderation policy that will only remove AI-generated content in specific instances, claiming that over-moderation of posts made with artificial intelligence was turning away human contributors.

The company also said in its post that a strict standard of evidence needed to be used moving forward in order to manage AI content, and that that standard of evidence hasn’t applied to most suspensions issued by moderators thus far. This directive was also communicated to the platform’s moderation team privately before being posted publicly. The moderators of the website are claiming that this directive will allow AI content, which can frequently be incorrect, to run rampant on the forum while expressing discontent with Stack Overflow for not communicating this new policy more effectively.

“Stack Overflow, Inc. has decreed a near-total prohibition on moderating AI-generated content in the wake of a flood of such content being posted to and subsequently removed from the Stack Exchange network, tacitly allowing the proliferation of incorrect information (“hallucinations”) and unfettered plagiarism on the Stack Exchange network. This poses a major threat to the integrity and trustworthiness of the platform and its content,” the mods write in their letter to Stack Overflow.

“A small number of moderators (11%) across the Stack Overflow network have stopped engaging in several activities, including moderating content. The primary reason for this action is dissatisfaction with our position on detection tools regarding AI-generated content,” Philippe Beaudette, VP of Community at Stack Overflow, said in a statement emailed to Gizmodo. “We stand by our decision to require that moderators stop using the tools previously used. We will continue to look for alternatives and are committed to rapid testing of those tools.”

Stack Overflow moderators, like those at Wikipedia, are volunteers tasked with maintaining the integrity of the platform. The moderators say that they tried to express their concerns with the company’s new policy through proper channels, but their anxieties fell on deaf ears. The mods plan to strike indefinitely, and will cease all actions including closing posts, deleting posts, flagging answers, and other tasks that help with website upkeep until AI policy has been retracted.

AI has been transforming Stack Overflow recently, for better or for worse. [Stack Overflow confirmed the Gizmodo that traffic was dropping due to OpenAI’s ChatGPT](https://gizmodo.com/stack-overflow-traffic-drops-as-coders-opt-for-chatgpt-1850427794) as more and more programmers began turning to the chatbot to debug their code as opposed to waiting for a human reply on the forum. Web analytics firm SimilarWeb reported in April that Stack Overflow has seen a drop in traffic every month since the beginning of 2022, with the average drop being 6%. In March, Stack Overflow saw a 13.9% drop in traffic from February and in April, the website saw 17.7% drop in traffic from March.",Stack Overflow Moderators Stop Work in Protest of Lax AI-Generated Content Guidelines,https://gizmodo.com/ai-stack-overflow-content-moderation-chat-gpt-1850505609
"[""Mike Stobbe""]",2023-07-24,2023-07-24,2023-06-01,2023-07-24,https://dims.apnews.com/dims4/default/34826ac/2147483647/strip/true/crop/1559x877+0+82/resize/1440x810!/quality/90/?url=https%3A%2F%2Fstorage.googleapis.com%2Fafs-prod%2Fmedia%2F213565956ce34165b638c70429d06caf%2F1559.jpeg,,,en,,apnews.com,"[""Khoa Lam""]","NEW YORK (AP) — Racial bias built into a common medical test for lung function is likely leading to fewer Black patients getting care for breathing problems, a study published Thursday suggests.

As many as 40% more Black male patients in the study might have been diagnosed with breathing problems if current diagnosis-assisting computer software was changed, the study said.

Doctors have long discussed the potential problems caused by race-based assumptions that are built into diagnostic software. This study, published in JAMA Network Open, offers one of the first real-world examples of how the the issue may affect diagnosis and care for lung patients, said Dr. Darshali Vyas, a pulmonary care doctor at Massachusetts General Hospital.

The results are “exciting” to see published but it’s also “what we’d expect” from setting aside race-based calculations, said Vyas, who was an author of an influential 2020 New England Journal of Medicine article that catalogued examples of how race-based assumptions are used in making doctors’ decisions about patient care.

For centuries, [some doctors and others have held beliefs](https://apnews.com/article/medical-racism-history-bb1e7d18a811c6c3878fcdb7095f434b) that there are natural racial differences in health, including one that Black people’s lungs were innately worse than those of white people. That assumption ended up in modern guidelines and algorithms for assessing risk and deciding on further care. Test results were adjusted to account for — or “correct” for — a patient’s race or ethnicity.

One example beyond lung function is a heart failure risk-scoring system that categorizes Black patients as being at lower risk and less likely to need referral for special cardiac care. Another is an equation used in determining kidney function that creates estimates of higher kidney function in Black patients.

The new study focused on a test to determine how much and how quickly a person can inhale and exhale. It’s often done using a spirometer — a device with a mouthpiece connected to a small machine.

After the test, doctors get a report that has been run through computer software and scores the patient’s ability breathe. It helps indicate whether a patient has restrictions and needs further testing or care for things like asthma, chronic obstructive pulmonary disorder or lung scarring due to air pollutant exposure.

Algorithms that adjust for race raise the threshold for diagnosing a problem in Black patients and may make them less likely to get started on certain medications or to be referred for medical procedures or even lung transplants, Vyas said.

While physicians also look at symptoms, lab work, X-rays and family histories of breathing problems, the pulmonary function testing can be an important part of diagnoses, “especially when patients are borderline,” said Dr. Albert Rizzo, the chief medical officer at the American Lung Association.

The new study looked at more than 2,700 Black men and 5,700 white men tested by University of Pennsylvania Health System doctors between 2010 and 2020. The researchers looked at spirometry and lung volume measurements and assessed how many were deemed to have breathing impairments under the race-based algorithm as compared to under a new algorithm.

Researchers concluded there would be nearly 400 additional cases of lung obstruction or impairment in Black men with the new algorithm.

Earlier this year, the American Thoracic Society, which represents lung-care doctors, issued a statement recommending replacement of race-focused adjustments. But the organization also put a call out for more research, including into the best way to modify software and whether making a change might inadvertently lead to overdiagnosis of lung problems in some patients.

Vyas noted some other algorithms have already been changed to drop race-based assumptions, including one for pregnant women that predicts risks of vaginal delivery if the mom previously had a cesarean section.

Changing the lung-testing algorithm may take longer, Vyas said, especially if different hospitals use different versions of race-adjusting procedures and software.","Black men were likely underdiagnosed with lung problems because of bias in software, study suggests",https://apnews.com/article/black-racial-bias-lung-medical-diagnosis-e1f73be6d00f17091600b6f21f20264d
"[""Jonathan Vanian""]",2023-07-24,2023-07-24,2023-06-07,2023-07-24,https://image.cnbcfm.com/api/v1/image/106804443-1606846321310-gettyimages-1229495348-Valera_Golovniov_IGOR5253.jpeg?v=1686175028&w=1920&h=1080,,,en,,cnbc.com,"[""Khoa Lam""]","An Instagram logo is seen displayed on a smartphone.

Instagram's recommendation algorithms have been connecting and promoting accounts that facilitate and sell child sexual abuse content, according to an investigation published Wednesday.

[Meta's](https://www.cnbc.com/quotes/META/) photo-sharing service stands out from other social media platforms and ""appears to have a particularly severe problem"" with accounts showing self-generated child sexual abuse material, or SG-CSAM, Stanford University researchers wrote in an accompanying study. Such accounts purport to be operated by minors.

""Due to the widespread use of hashtags, relatively long life of seller accounts and, especially, the effective recommendation algorithm, Instagram serves as the key discovery mechanism for this specific community of buyers and sellers,"" according to the study, which was cited in the [investigation](https://www.wsj.com/articles/instagram-vast-pedophile-network-4ab7189) by The Wall Street Journal, Stanford University's Internet Observatory Cyber Policy Center and the University of Massachusetts Amherst.

While the accounts could be found by any user searching for explicit hashtags, the researchers discovered Instagram's recommendation algorithms also promoted them ""to users viewing an account in the network, allowing for account discovery without keyword searches.""

A Meta spokesperson said in a statement that the company has been taking several steps to fix the issues and that it ""set up an internal task force"" to investigate and address these claims.

""Child exploitation is a horrific crime,"" the spokesperson said. ""We work aggressively to fight it on and off our platforms, and to support law enforcement in its efforts to arrest and prosecute the criminals behind it.""

Alex Stamos, Facebook's former chief security officer and one of the paper's authors, said in a [tweet](https://twitter.com/alexstamos/status/1666490180910608384) Wednesday that the researchers focused on Instagram because its ""position as the most popular platform for teenagers globally makes it a critical part of this ecosystem."" However, he added ""Twitter continues to have serious issues with child exploitation.""  

Stamos, who is now director of the Stanford Internet Observatory, said the problem has persisted after [Elon Musk](https://www.cnbc.com/elon-musk/) acquired Twitter late last year.

""What we found is that Twitter's basic scanning for known CSAM broke after Mr. Musk's takeover and was not fixed until we notified them,"" Stamos wrote.

""They then cut off our API access,"" he added, referring to the software that lets researchers access Twitter data to conduct their studies.

Earlier this year, NBC News [reported](https://www.nbcnews.com/tech/tech-news/musk-twitter-elon-child-abuse-material-rcna63621) multiple Twitter accounts that offer or sell CSAM have remained available for months, even after Musk pledged to address problems with child exploitation on the social messaging service.

Twitter didn't provide a comment for this story.","Instagram's algorithms are promoting accounts that share child sex abuse content, researchers find",https://www.cnbc.com/2023/06/07/instagram-promotes-accounts-sharing-child-sex-abuse-content-research.html
"[""Andy Nghiem""]",2023-07-24,2023-07-24,2023-05-18,2023-07-24,https://jnswire.s3.amazonaws.com/jns-media/63/14/11416220/closeup-of-gavel-1600x900.jpg,,,en,,madisonrecord.com,"[""Khoa Lam""]","EDWARDSVILLE - A class action lawsuit alleges a facial recognition search engine that searches the internet for peoples' photos invades their privacy in violation of Illinois law.

Plaintiffs Amy Newton, Amanda Curry, Manuel Clayton, Misty McGraw, and Nicholas Clayton filed a class action lawsuit in the Madison County Circuit Court against Pimeyes Sp. Z O.O, Transaction Cloud, Inc., Face Recognition Solutions, LTD., Carribex LTD., EMEA Robotics, LTD., Public Mirror SP. Z O.O, Lukasz Kowalczyk, Denis Tatina, Giorgi Gobronidze, and Does 1-25, citing negligence and carelessness in violation of the Illinois Biometric Information Privacy Act (BIPA).

According to the lawsuit, defendants offer a service called Pimeyes, a facial search engine that can search the internet for photos that contain a given face. The company charges users $29.99 per month for this service. 

The lawsuit alleges that to build this powerful search engine, the defendants have collected images of millions of Americans from databases across the internet and scanned them using artificial intelligence into their own database without consent.

According to the lawsuit, many of these databases contain numerous images of the plaintiffs. This database is massive and continuously updated with new photos, the lawsuit states.

The suit adds that the company's service can be used by anyone to search for images of anyone else to download without that person's consent. 

The lawsuit states that BIPA laws require that prior to collecting biometric data, companies must inform visitors in writing that biometric data will be collected and stored. It also states that visitors must be informed in writing of the specific purpose of why the biometric data is being collected, how long it will be stored, and companies must receive a written release from visitors for the collection of biometric data.

The plaintiffs argue that the defendants invaded their privacy and the privacy of untold Americans by collecting and storing their biometric data without informed consent. They add that Pimeyes does not have written, publicly available policies identifying how long it will store biometric data or information for permanently destroying biometric data. 

The plaintiffs are demanding a jury trial to seek damages for themselves and everyone in the class action lawsuit, plus court costs, attorney fees and any other relief the court deems proper. They are also requesting the court to issue an order requiring the defendant to comply with BIPA and cease the collection of biometric data without informed written consent. They are represented in this case by the attorneys of Peiffer, Wolf, Carr, Kane, Conway & Wise, LLP in St. Louis. 

_Madison County Circuit Court case number 2023LA000629_",Illinois residents allege facial image search engine violates BIPA,https://madisonrecord.com/stories/642174142-illinois-residents-allege-facial-image-search-engine-violates-bipa
"[""Charles Rollet""]",2023-07-24,2023-07-24,2023-05-30,2023-07-24,https://s.ipvm.com/uploads/04ab/302c/4a2d18e7-bfae-4f31-9795-05ff53ea33ee.jpg,,,en,,ipvm.com,"[""Khoa Lam""]","**""Banner\_Alarm"" \*\* \*\* \*\*\*\*\*\*\*\***

\*\*\*\*\*\*\*\* \*\* \*\*\*\*, \*\*\*\*\*'\*[""\*\*\*\*"" \*\* \*\*\*\*\*\*\*\*](#restricted)(大华巨灵\*\*开放平台), \*\*\*\*\*\* \*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\* \*\* detecting \*\*\*\* \*\*\*\* \*\* \* \*\*\*\*\*\*\*\*\*\*\*\* site, \*\*\*\*\*\* \*\*\*\*\*\*\*, \*\*\*\*\*\* \*\*\*\* \*\*\*\*, etc.

[\*\*\* \*\*\*\*\*\*\*\*, \*\*\*\*\*\* ""\*\*\*\*\*\* \*\*\*\*\*\*"" (拉横幅)](#restricted), \*\* \*\*\*\*\*\* \*\*\*\*\* \*\*\* ""\*\*\*\*\*\* safety"" (社会治安) \*\*\*\*\*\*\*\* \*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\*\* at \*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\* \*\*\*\*:

> \*\* \*\*\* \*\*\*\*\*\*\*\*\*\* \*\*\*\*, \*\***a \*\*\*\*\*\* \*\*\*\*\*\*\* \* \*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\*** and lasts for a certain period of time, **an \*\*\*\*\* \*\*\*\* \*\* \*\*\*\*\*\*\*\*\*** \[emphasis added\]
> 
> 指定区域内，检测到人举横幅且持续一定时间则产生报警

\*\*\*\*\* \*\*\* \* \*\*\*\* \*\* \*\*\* system \*\*\*\* \*\*\* \*\*\*\*\*\*\* \*\*\*\*\* ""\*\*\*\*\*\*\*\*\*\*\*\*"" on \*\*\* \*\*\* \*\*\*\*. \*\*\* \*\*\*\* also \*\*\*\*\* \*\*\*\* \*\*\*\*\*\*\*\*\*\*' \*\*\*\*\* \*\*\* automatically \*\*\*\*\*\*\*\* \*\*\* \*\*\*\*\*\*\*\* \*\* \*\*\*\* of \*\*\* \*\*\*\* \*\*\*\*\*\*\*\*:



\*\*\* \*\*\*\*\*\* \*\*\*\*\*: ""\*\*\*\*\* \*\*\* \*\* days, \*\*\*\*\* \*\* \*\*\* \*\*\*\* \*\*\*\*\*\*, Win"" (战\*\*天, 拼在寒冬, 赢下). \*\*\*\* \*\*\*\*\* not \*\*\*\* \*\*\* \*\*\*\*\*\*\* \*\* \*\*\*\* slogan \*\*\*\*\*\*\* \*\* \*\*\*\* \*\*\*\*\* \*\*\*\*. It \*\* \*\*\*\*\*\*\* \*\* \*\*\*\* \*\*\*\*\*\* to \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\* \*\* \*\* \*\*\*\* invented \*\*\* \*\*\*\* \*\*\*\*\*\*\*\*.

**No \*\*\* \*\*\*\*\*\*\*\***

\*\*\*\*\*'\* \*\*\*\*\*\* \*\*\*\* \*\*\* \*\*\*\*\*\*\* \*\*\*\*\*\*\* character \*\*\*\*\*\*\*\*\*\*\* (\*\*\*) \*.\*. \*\*\*\*\* \*\* no \*\*\*\*\*\*\*\* \*\*\*\* \*\*\* \*\*\*\*\*\* \*\*\* automatically \*\*\*\*\*\*\*\* \*\*\*\* \*\*\* \*\*\*\*\*\* \*\* the \*\*\*\*\*\* \*\* \*\* \*\* \*\*\* to \*\*\*\*\* \*\*\*\* \*\* \*\*\*\*\*\*\*\* \*\*\*\*\* or \*\*\*\*\*\*\*. \*\*\*\*\*\*\*\*\*\*\*\*, \*\*\*\*\*\*\*\*\*\*\*, \*\*\*\* \*\*\*\*\* not \*\* \*\*\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\*\* as \*\*\* \*\* \*\*\*\* \*\*\*\*\*\*\*\*\*\*\* \*\*\* banners \*\*\*\* \*\* \*\* \*\*\*\*\*, \*\*\*\*\*\* it \*\*\*\* \*\*\* \*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*.

**Intended \*\*\* \*\*\* \*\*\*\*\*\*\*\*\*\*\***

\*\*\*\*\*'\* \*\*\* \*\* \*\*\* \*\*\*\*\* ""\*\*\*\*\*\* safety"" (社会治安) \*\*\* ""\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*\*"" (社会治理) to \*\*\*\*\*\*\*\* \*\*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\* police \*\*\* \*\*\*\*\* \*\*\* \*\*\*\*\*\*\*\*\*\*\* \*\*\* targeted \*\*\*\*\* \*\*\*\*\* \*\*\*\*\* \*\*\*\*\* \*\*\*[\*\*\*\*\*\*\*\*\*\*\*\* \*\*\*\* \*\* \*\*\*\*\*](#restricted)\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\* \*\*\*\*\* \*\*\*\* enforcement[\*\*\*\*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*.](#restricted)

**Demo \*\*\*\* \*\*\*\*, \*\*\*\*\*\*\*\*\* \*\* \*\*\*\***

\*\*\*\* \*\*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\*\*[\*\*\*\* \*\*\* \*\*\*\*\*\*\*\*\* \*\* \*\*\* \*\*\*\*](#restricted)\*\*\*\*\*\* \*\*\*\* \*\*\* \*\*\*\* \*\*\*\* \*\*\*\*\*\*\* for \*\*\*\*\* \*\* \*\*\* \*\*\*\*\*\* \*\*\*\*\* was \*\*\*\*\*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\* \*\*\*\*.

**Intended \*\*\* \*\*\*\* \*\*\*\*\*\*\*, \*\*\*\*\*, \*\*\*\*\***

[\*\*\*\*\*'\* \*\*\*\*\*\*\*\*](#restricted)\*\*\*\*\*\* \*\* \*\* \*\*\*\*\*\*\*\* \*\*\* ""\*\*\*\*\*\* halls \*\* \*\*\*\*\*\*\*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\* \[\*\*\*\*\] squares, \*\*\*\*\*, \*\*\* \*\*\*\*\* \*\*\*\*\*\*"" (室内大厅或室外较空旷的广场，马路等场景).

**Targets \*\*\*\*\*\*\* \*\*\*-\*\*\* \*\*\*\*\*\***

[\*\*\*\*\*'\* \*\*\*\*\*\*\*\*](#restricted)\*\*\*\* \*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\* ""\*\*\*\*\*\*\* or \*\*\*\*\*\*\*\*"" \*\*\*\* \*\*\*\*\*\* \*\*\*-\*\*\* \*\*\*\*\*\* in \*\*\*\* \*\* \*\*\*\*\* \*\*\*\*\*\*\*\*\*\* (常见的多边形横幅或标语牌，目标大小在\*\*\*\*\*下\*\*\*~\*\*\*像素).

**Infinova, \*\*\*\*\*\* \*\*\*\* \*\*\*\* \*\*\*\*\*\*\***

\*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\* \*\*\*\* \*\*\*\*\*\*\* ""\*\*\*\*\*\*\*\* banner \*\*\*\*\*\*\*\*\*"" (拉横幅检测) \*\*\*\*\*\*\*\*\*\*\*\*: \*\*\*[\*\*\*\*\*-\*\* \*\*\* \*\*\*\*\*\* \*\*\*\*\*\*](#restricted)\*\*\* \*\*\*[\*\*\*\*\*-\*\*\*\*-\*\*\*-\* \*\*\* \*\*\*\*\*\* \*\*\*\*\*\*:](#restricted)

> \*\*\*\*\*\*\* \*\*\*\* \*\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*
> 
> \* \*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*: \*\*\*\*\*\*\*\* \*\*\*\*\*\*\* from \*\*\*\* \*\*\*\*\*\*\*\*\*,**unfurl \*\*\*\*\*\* \*\*\*\*\*\*\*\*\***
> 
> 支持丰富的智能分析功能
> 
> \*项行为分析检测：高空抛物、拉横幅检测 \[\*\*\*\*\*\*\*\* \*\*\*\*\*\]

\*\*\*\*\* \*\*\*\*\*\*\* \*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\* \*\*\*\* analytics \*\*\*, \*\*\*\*[\*\* \*\*\*\*\* \*\*\*\*](#restricted)\*\*\* \*\*\*[\*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\* \*\*\*\*.](#restricted)

**Protesting \*\* \*\*\*\*\* \*\*\*\*\*\*\*\*\*\***

\*\*\*\*\* \*\*\*\*\*\*\* \*\* \*\* \*\*\*[\*\*\* \*\*\*\*\*\*\*\*\*\*\*\*](#restricted)\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\*, \*\*\*\*\*\*\*\*\*\* \*\*\* those \*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\* \*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\* due \*\* \*\*\*\*\*\*\*\*\*[\*\*\* \*\*\*\*](#restricted)\*\*\*\*\*\*\* ""\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\* \*\*\*\*\*"" \*\*\* ""\*\*\*\*\*\*\*\*\* social \*\*\*\*\*\*\*\*\*\*\*\*\*\*"",[\* \*\*\* \*\*\*\*\*\* \*\*\*\*\*\*\*\*.](#restricted)

\*\*\*\*\*, \*\*\*\*\*\*\*\*\* \* \*\*\*\*\*\* (""拉横幅"") \*\* frequently \*\*\*\* \*\* \*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*\* \*.\*. this[\*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\* \*\*\*\*\*\*\*](#restricted)\*\*\*\* \*\*\*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*\*\* ""\*\*\*\*\*\* teachers' \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\* \*\* \*\*\* \*\*\*\*"" (依法保障教师待遇):



\*\*\*\* \*\*\*\*\*\*\*\*, \*\*\*\*\*'\* \*\*\*\*'\* \*\*\*\*-\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*[\*\*\*\* \*\*\*\*\*\*\*\*](#restricted)\*\* \* \*\*\* \*\*\*\*\*\*\*\*\* \* \*\*\*\*\*\* in \*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\* ""\*\* \*\*\*'\* \*\*\*\* lockdowns, \*\* \*\*\*\* \*\*\*\*\*\*\*"" (不要封控要自由). \*\*\* protestor \*\*\* \*\*\*\*\*\*\*\* \*\* \*\*\* \*\*\*\* and \*\*\*[\*\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\* \*\*\*\*\*\*\*.](#restricted)



**PRC \*\*\*\*\*\*\* \*\*\*\* \*\*\*\*\*\*\***

\*\*\*\*\*\* \*\* \*\*\* \*\* \*\*\* \*\*\*\*\*\*, where \*\*\*\*\*\*\* \*\*\*\*\* \*\*\*\* \*\* \*\*\* shapes \*\*\* \*\*\*\*\*, \*\* \*\*\* \*\*\* protest \*\*\*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\*, making \*\* \*\*\*\*\*\* \*\* \*\*\*\*\* \*\*\*\*\* analytics \*\*\*\* \*\*\* \*\*\*\*\*\*\*\*\*\*\*\*\* \*\*\*\*\*\* \*\*\*\*. IPVM \*\*\*\*\*[\*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*](#restricted)\*\*\*\* \*\*\* \*\*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\* explicitly \*\*\* \*\*\*\*\*\*\*\* \*\*\*\* \*\*\*\*\*\*\*\*\*:



**Hikvision \*\*\*\*\*\*\*\*\*\***

\*\*\*\*\*\*\*\*\* \*\*\* \*\*\*\* \*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\*\* to \*\*\* \*\*\*\*\*\*:

*   \* \*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\* \*\* \*\*\*\*\* for ""\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*\*, \*\*\*\*\*\*\*\*\*\*\*, \*\*\*\*\*\*\*\*\*\*\*\*\*\*"" (非法集会、游行、示威) and \*\*\*\*\*\* \*\*\*\*\*\*\* \*\* ""\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*"" (聚众),[\*\*\*\*](#restricted)\*\*\*[\*\*\* \*\*\*\*\*\*\*\*](#restricted)\*\*\*\*\*\*\*\* \*\* \*\*\*\*.
*   \*\*\*\*\*\*\*\*\* \*\*\* \* \*\*\*\* \*\*\*\*\*\*\*\* \*\* build \* \*\*\*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\*\* \*\*\* risk \*\* \*\*\*\*\*\*\*\*\*\*/""\*\*\*\*\*\*\*\*\*\*\*"" (信访人) \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*,[\*\*\*\*](#restricted)\*\*\*[\*\*\* \*\*\* \*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\*\*.](#restricted)

\*\*\*\*\*\*\*, \*\*\*\*\*\*\*\*\*'\* \*\*\*\*\*\* \*\*\* \*\*\* \*\*\*\*\*\*\* by \*\*\*\*\* \*\*\*\*\*\*\*\*\*, \*\*\*\*\*\*\*, \*\*\*\* \*\*\*\*\*\*\*\*\* with \*\*\*\*\*\*\*\* \*\*\*\*\*\* \*\*\*\*\*\*\* \*\*\* \*\*\*\*\*\*\*\* protesters \*\*\* \*\*\*\*\*\*\*\*\*\*\*\* ""\*\*\*\*\*\*\*\*\*\*\*"" (信访人), \* group \*\*\*\* \*\*[\*\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*\* \*\* \*\*\* \*\*\*.](#restricted)

**Dahua \*\*\*\*\*\*\* \*\*\*\*\*\*\*\***

\*\*\*\*\*\* \*\*\*\* \*\*\*\*\*\*\* \*\*\* \*\* \*\*\*\*\*, the ""\*\*\*\*\*\* \*\*\*\*\*\*"" (社会治安) \*\*\*\*\*\*\*\* \*\*\*\*\*\* the \*\*\*\*\*\*\*\*\* \*\* ""\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*"" (拉横幅):



\*\*\*\*\*\*\*, \*\*\*\*\* \*\*\*\*\*\*\* \*\*\* \*\*\*\*\*\*\*\* \*\*\* its \*\*\*\*\*\*\* \*\*\*\*\*\*\* \*\*\*\*\* \*\*\*\* \*\*\*\*\*\*\* out \*\*\* \*\*\*\*\*\*\*[\*.\*., ""\*\*\*\*\*\*\*\* \*\*\*\*\*\*\*"" (拉横幅) \*\* \*\*\*\*\*\* appears](#restricted). \*\*\*\* \*\*\*\*\*\*\*\* \*\*\*\* \*\*\*[\*\*\*\*\*\*\*\* \*\*\* \*\*\*\*\* \*\*\*\*\*\*\*.](#restricted)

\*\*\*\* \*\* \*\*\*\*\*\*\* \*\*\* \*\*\* \*\*\*\*\* which \*\*\* \* \*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\* evidence \*\*\* \*\*\* \*\*\* \*\*\*\*\*\*\*, \*.\*.[\*\*\*\*\* \*\*\*\*\* \*\* \*\*\*\*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\*, Then \*\*\*\*\* (\*\*\*\*)](#restricted)\*\*\*[\*\*\*\*\*'\* \*\*\*\*\*\* \*\*\*\*\* \*\*\*\* \*\*\*, \*\*\*\*\*\*\* Evidence (\*\*\*\*).](#restricted)

**Dahua, \*\*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\***

\*\*\*\*\*\*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\* \*\*\*\*\*\*\*\* \*\*\* \*\*\*\* article. \*\*\*\* \*\*\*\*\* \*\*\*\*\*\*\* \*\*\* \*\* the \*\*\*\*\*\*\*\*\* \*\* \*\*\* \*\*\*\* (\*\*\*\*\*) and \*\*\* \*\*\*\* (\*\*\*\*\*\*\*\*), \* \*\* 5 \*\*\*\* \*\*\*\*\*\* \*\* \*\*\*\*\*\*\*\*\*.

\*\*\*\*\*\*: \*\*\*\*\*\*\*\*'\* \*\*\*\*\*\*\*\*\* \*\*\*\* \*\*\* \*\*\*\* she \*\* ""\*\*\* \*\*\*\*\*"" \*\* \*\*\*\*\*\*\* with \*\*\*\*\*\* \*\*\*\*\*\* \*\*\*\*\* \*\*\*\* \*\* police \*\*\* \*\*\*\*\*\*\*\*'\* \*\*\* \*\*\*\*\* \*\*\* only \*% \*\* \*\*\*\*\* \*\*\*\*\*\*\*. \*\*\*\*, Infinova \*\*\*\* \*\*\*\*\* $\*\*\*\* \*\*\* \*\* sales[\*\*\* \*\*\*\*](#restricted)\*\* \*\*\*\* \*\*\*\*\*\*\*\*\*\* \*\*\*\*\* $\*.\*\* \*\* sales.

\*\*\*\* \*\*\* \*\*\*\* \*\*\*\*\*\*\*\*\* \*\*\*\*\*:

> \* \*\* \*\*\* \*\*\*\*\* \*\* \*\*\*\*\* of \*\*\*\*\* \*\*\*\*\*\*\* \*\* \*\*\*\*\*\*. \*\* general, \*\*\*\*\*\*\*\*’\* \*\*\*\*\*\* \*\*\*\*\* \*\* \*\*\*\*\* are \*\*\*\* \*\*\* (\*\*\*\*\* \*% \*\* total \*\*\*\*\*\*\*).

\*\*\*\*\* \*\*\* \*\*\*\* \*\*\* \*\* ""\*\*\* aware"" \*\* \*\*\*\*\* \*\* \*\*\*\*\*\*, \*\*\*\*\*\* detection \*\*\* \*\* \*\*\* \*\*\*\*\* (\*\*\*\* IPVM \*\*\*\*\* \*\*\*\*) \*\*\*\*\*\*\* \*\* \*\*\* law \*\*\*\*\*\*\*\*\*\*\*. \*\*\* \*\*\*\*\* \*\*\*\*\*\*\*\*'\* \*\*\*\*\* sales \*\*\* \*\*\*, \*\*\*\* \*\*\*\* \*\*\* or \*\*\*\*\*\*\* \*\*\*\*\*\*\* \*\*\*\* \*\*\*\* \*\*\*\*\*\*\*\*\*\* could \*\* \*\*\*\* \*\* \*\*\*\*\* \*\* protestors.","Dahua Selling Protestor / Banner Alarms, Deletes Evidence",https://ipvm.com/reports/dahua-protestor-alarms
"[""Smitha Milli"",""Micah Carroll"",""Sashrika Pandey"",""Yike Wang"",""Anca D. Dragan""]",2023-07-24,2023-07-24,2023-05-26,2023-07-24,https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png,,,en,,arxiv.org,"[""Khoa Lam""]","As social media continues to have a significant influence on public opinion, understanding the impact of the machine learning algorithms that filter and curate content is crucial. However, existing studies have yielded inconsistent results, potentially due to limitations such as reliance on observational methods, use of simulated rather than real users, restriction to specific types of content, or internal access requirements that may create conflicts of interest. To overcome these issues, we conducted a pre-registered controlled experiment on Twitter's algorithm without internal access. The key to our design was to, for a large group of active Twitter users, simultaneously collect (a) the tweets the personalized algorithm shows, and (b) the tweets the user would have seen if they were just shown the latest tweets from people they follow; we then surveyed users about both sets of tweets in a random order.  

Our results indicate that the algorithm amplifies emotional content, and especially those tweets that express anger and out-group animosity. Furthermore, political tweets from the algorithm lead readers to perceive their political in-group more positively and their political out-group more negatively. Interestingly, while readers generally say they prefer tweets curated by the algorithm, they are less likely to prefer algorithm-selected political tweets. Overall, our study provides important insights into the impact of social media ranking algorithms, with implications for shaping public discourse and democratic engagement.","Twitter's Algorithm: Amplifying Anger, Animosity, and Affective Polarization",https://arxiv.org/abs/2305.16941
"[""Gavin Abercrombie"",""Amanda Cercas Curry"",""Tanvi Dinkar"",""Zeerak Talat""]",2023-07-24,2023-07-24,2023-05-16,2023-07-24,https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png,,,en,,arxiv.org,"[""Khoa Lam""]","Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism is inevitable, conscious and unconscious design choices can guide users to personify them to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to transparency and trust issues, and high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have begun to investigate factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be considered. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, arguing that it can reinforce stereotypes of gender roles and notions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.",Mirages: On Anthropomorphism in Dialogue Systems,https://arxiv.org/abs/2305.09800
"[""Avram Piltch""]",2023-07-24,2023-07-24,2023-06-11,2023-07-24,https://cdn.mos.cms.futurecdn.net/cE4DTLrJF5ueefP3LSooaY-1200-80.png,,,en,,tomshardware.com,"[""Khoa Lam""]","Search has always been the Internet’s most important utility. Before Google became dominant, there were many contenders for the search throne, from Altavista to Lycos, Excite, Zap, Yahoo (mainly as a directory) and even Ask Jeeves. The idea behind the World Wide Web is that there’s power in having a nearly infinite number of voices. But with millions of publications and billions of web pages, it would be impossible to find all the information you want without search. 

Google succeeded because it offered the best quality results, loaded quickly and had less cruft on the page than any of its competitors. Now, having taken over [91 percent of the search market](https://kinsta.com/search-engine-market-share/#:~:text=Google%20dominates%20the%20search%20engine,91.88%25%20as%20of%20June%202022.), the company is testing a major change to its interface that replaces the chorus of Internet voices with its own robotic lounge singer. Instead of highlighting links to content from expert humans, the “Search Generative Experience” (SGE) uses an AI plagiarism engine that grabs facts and snippets of text from a variety of sites, cobbles them together (often word-for-word) and passes off the work as its creation. If Google makes SGE the default mode for search, the company will seriously damage if not destroy the open web while providing a horrible user experience.

A couple of weeks ago, Google made SGE available to the public in a limited beta (you can [sign up here](https://www.tomshardware.com/how-to/access-google-ai-betas)). If you are in the beta program like I am, you will see what the company seems to have planned for the near future: a search results page where answers and advice from Google take up the entire first screen, and you have to scroll way below the fold to see the first organic search result.  

For example, when I searched “best bicycle,” Google’s SGE answer, combined with its shopping links and other cruft took up the first 1,360 vertical pixels of the display before I could see the first actual search result. 



(Image credit: Tom's Hardware)

For its part, Google says that it’s just “experimenting,” and may make some changes before rolling SGE out to everyone as a default experience. The company says that it wants to continue driving traffic offsite.

“We’re putting websites front and center in SGE, designing the experience to highlight and drive attention to content from across the web,” a Google spokesperson told me. “SGE is starting as an experiment in Search Labs, and getting feedback from people is helping us improve the experience and understand how generative AI can be helpful in information journeys. The experiences that ultimately come to Search will likely look different from the experiments you see in Search Labs. As we experiment with new LLM-powered capabilities in Search, we'll continue to prioritize approaches that will drive valuable traffic to a wide range of creators."" 

By “putting websites front-and-center,” Google is referring to the block of three related-link thumbnails that sometimes (but not always) appear to the right of its SGE answer. These are a fig leaf to publishers, but they’re not always the best resources (they don’t match the top organic results) and few people are going to click them, having gotten their “answer” in the SGE text.



(Image credit: Tom's Hardware)

For example, when I searched for “Best CPU,” the related links were from the sites [Maketecheasier.com](https://www.maketecheasier.com/buying-cpu-processor-guide/#:~:text=Clock%20speed%20explains%20how%20fast%20the%20individual,count%20and%20vice%20versa%20for%20productivity%20users.), [Nanoreview](https://nanoreview.net/en/cpu-compare/amd-ryzen-9-7900x3d-vs-amd-ryzen-7-7800x3d#:~:text=Pros:%20+%20Has%204%20more%20physical%20cores,v5%20test%20%2D%202192%20vs%201945%20points) and [MacPaw](https://macpaw.audw.net/c/221109/66209/1733?subId1=tomshardware-us-7035733169633876000&sharedId=tomshardware-us&u=https%3A%2F%2Fmacpaw.com%2Fhow-to%2Fchoose-best-processor-for-mac). None of these sites is even on the first page of organic results for “Best CPU” and for good reason. They aren’t leading authorities in the field and the linked articles don’t even provide lists of the best CPUs. The MacPaw article is about how to choose the best processor for your MacBook, a topic that does not match the intent of someone searching for “best CPU,” as those folks are almost certainly looking for a desktop PC processor.

A Plagiarism Stew
-----------------

Even worse, the answers in Google’s SGE boxes are frequently plagiarized, often word-for-word, from the related links. Depending on what you search for, you may find a paragraph taken from just one source or get a whole bunch of sentences and factoids from different articles mashed together into a plagiarism stew. 

When I searched “which is faster the Ryzen 7 7800X3D or the Core i9-13900K,” the Google SGE grabbed an exact phrase from our Tom’s Hardware [article comparing the two CPUs](https://www.tomshardware.com/news/amd-ryzen-7-7800x3d-vs-intel-core-i9-13900k-vs-intel-core-7-13700K#:~:text=In%20our%20test%20suite%2C%20the,and%209%25%20faster%20at%201440p.), writing “The Ryzen 7 7800X3D is 12% faster than the Core i9-13900K at 1080p gaming and 9% faster at 1440p.” It then rephrased two sentences from [this article on Hardware Times](https://www.hardwaretimes.com/amd-ryzen-7-7800x3d-vs-intel-core-i9-13900k-12-gaming-benchmarks-power-efficiency-temps/#:~:text=The%20Core%20i9%2D13900K%20snags,13900K%20in%20Ubisoft's%20latest%20title.). The original copy read as:

“The Core i9-13900K snags a win in “A Plague Tale” both with and without ray-tracing. It’s marginally faster than the Ryzen 7 7800X3D with similar lows. The tables get turned in Assassins’ Creed Valhalla as the 7800X3D edges past the 13900K in Ubisoft’s latest title.”

And Google’s AI wrote it as:

“The Core i9-13900K is marginally faster than the Ryzen 7 7800X3D in ‘A Plague Tale’. However, the Ryzen 7 7800X3D edges past the Core i9-13900K in Assassins' Creed Valhalla.”



(Image credit: Tom's Hardware)

You can even clearly see in our screenshot that our sentence is quoted word-for-word in Google’s “featured snippet” box but not in the SGE box (which will likely replace the featured snippets in the future since SGE does basically the same thing). Yes, both the Hardware Times article and the Tom’s Hardware article that Google’s bot copied data from are listed as related links on the right side of the box. 

When I asked Google about the fact that its SGE answers are frequently word-for-word copies drawn from the related links articles, the company said that it picks those links because they “corroborate” the responses.

“Generative responses are corroborated by sources from the web,” the spokesperson said. “And when a portion of a snapshot briefly includes content from a specific source, we will prominently highlight that source in the snapshot.”

It’s pretty easy to find sources that back up your claims when your claims are word-for-word copied from those sources. While the bot could do a better job of laundering its plagiarism, it’s inevitable that the response would come from some human’s work. No matter how advanced LLMs get, they will never be the primary source of facts or advice and can only repurpose what people have done. LLMs are relatively good at generating “creative” works that are designed to be a mashup of existing ideas (ex: “write me a haiku about farts”) but, until they are connected to robotic bodies that go out and gather information first-hand, they will never be a source of truth.

The company also said that “you can expand to see how the links apply to each part of the snapshot.” There’s an expand icon that sits inconspicuously in the upper right corner of the SGE box, above the third related link. And, if you decide to click it, you will see a clunky interface which puts the thumbnails for related links inline with the pilfered text.

Image 1 of 2



(Image credit: Tom's Hardware)



(Image credit: Tom's Hardware)

Whether you click the expand button or not, SGE’s related links are not presented as citations, but recommendations for further reading. If I start singing “Thriller” then tell you that it’s an original song I wrote, it doesn’t matter if I also say “you might want to listen to a guy named Micheal Jackson because he also makes some nice songs like this.” That’s still plagiarism and, even if it were not, we’d have a problem. 

Plagiarism is a moral and academic term, not a legal one, and simply giving credit is not a defense against copyright infringement. You can’t run a business selling pirated Blu-ray discs and then, when busted, say “it’s all good, because I listed George Lucas as the director of _Star Wars_ rather than substituting my own name in the credits.”

In answering my questions, Google’s spokesperson also compared the SGE box to [featured snippets](https://support.google.com/websearch/answer/9351707?hl=en), noting that publishers today usually want their articles to appear in featured snippets because those links drive traffic back. While both experiences use content directly from publishers, featured snippets are short quotes with direct attribution and a very prominent link directly to the source. They do not pretend to be generated by an all-knowing AI and they often give you just enough information to want to click-through for more.

From a reader’s perspective, we’re left without any authority to take responsibility for the claims in the bot’s answer. Who, exactly, says that the Ryzen 7 7800X3D is faster and on whose authority is it recommended? I know, from tracing back the text, that Tom’s Hardware and Hardware Times stand behind this information, but because there’s no citation, the reader has no way of knowing. Google is, in effect, saying that its bot is the authority you should believe. 

The fallacy underlying Google SGE is the false belief that a bot can have authority in the first place. Until the bot grows a pair of hands and opens its own lab space, it will never test CPUs. Until it opens a kitchen, it will never have its own family recipes. The only thing it can cook up is a plagiarism stew. 

Relying on an unsourced bot as the end-all, be-all authority stands in direct contradiction to Google’s stated [emphasis on E-E-A-T](https://developers.google.com/search/blog/2022/12/google-raters-guidelines-e-e-a-t) (Expertise, Experience, Authority and Trust), a standard it uses to decide which websites and authors should rank highly in organic search. 

It makes total sense that someone who has been reviewing CPUs for 15 years on a website that specializes in CPUs should have their AMD Ryzen review rank higher than someone with no authority on the topic. Unfortunately, when it comes to Google’s own AI author – a faceless entity that has no experience doing anything – the rules go out the window.

Mish-Mash Plagiarism Leads to Poor Answers
------------------------------------------

At least the result we got when asking which CPU was faster was an accurate one. However, by mashing up text from different sources and then not sharing what the source for each sentence or bullet point is, Google is offering incorrect information that often contradicts the source material it’s copied from, or contradicts itself.

For example, I searched for “ThinkPad X13 AMD Review,” because I was interested in seeing what reviewers thought of Lenovo’s ThinkPad X13 laptop with AMD processor inside. The Google bot wrote its own mini-review, complete with bulleted pros and cons for the ThinkPad X13, while grabbing sentences and bullet points from at least four different articles, including [a review](https://www.laptopmag.com/reviews/lenovo-thinkpad-x13-amd) from Laptop Mag, [a review](https://www.tomshardware.com/reviews/lenovo-thinkpad-X13-gen1-amd) from Tom’s Hardware, [another review](https://www.notebookcheck.net/Lenovo-ThinkPad-X13-Gen-2-review-AMD-Ryzen-Pro-makes-the-compact-business-laptop-fast.580644.0.html#:~:text=Pros:%20+%20Very%20good%20system%20performance%20Cons:,slow%20for%20competitive%20gamers%20%2D%20Not%20great) from Notebook Check and a [blog post](https://go.redirectingat.com/?id=92X1584492&xcust=tomshardware_us_8146957167845176000&xs=1&url=https%3A%2F%2Fwww.laptopoutlet.co.uk%2Fblog%2Fbest-laptops-for-civil-and-structural-engineers.html%23%3A~%3Atext%3DPacked%2520with%2520high%252Dperforming%2520AMD%2520Ryzen%25207%2520PRO%2Cthe%2520best%2520and%2520cheapest%2520laptops%2520out%2520there.&sref=https%3A%2F%2Fwww.tomshardware.com%2Fnews%2Fgoogle-sge-break-internet) from LaptopOutlet – which is a store that had about 100 words on the product. 

The image below shows the result, along with pointers to where the SGE took its content from.



(Image credit: Tom's Hardware)

Aside from it being plagiarism and a slap in the face to the writers who did the actual work of testing and using this laptop, Google’s answer has a lot of issues. First of all, the answer refers to the ThinkPad X13 Gen 3 (the latest version with AMD CPU) but the reviews it draws from are from the Gen 1 and Gen 2 versions of the product, which are not the same.

While Laptop Mag and Tom’s Hardware both praised the laptop’s keyboard and durable design, both sites described the battery life as “lackluster” or “subpar,” while Google lists “Long battery life” as a pro. The bot clearly got the battery life pro from another site, but by mixing advice from different sources, Google is presenting readers with a very inaccurate picture. 

Also, since the bot doesn’t cite sources, the reader has no way to know who thought it had long battery life, whether that came from a reputable outlet and how they tested. One of the sources, LaptopOutlet, is a store that sells laptops and doesn’t do any benchmark testing. Should its claims be given equal weight to those journalists who actually do test and aren’t actively trying to sell the product? Like most LLMs, Google’s SGE bot doesn’t seem to care whether it’s giving you the truth or just mashing sentences together in a way that seems convincing.

Giving Faulty Medical Advice
----------------------------

The Google SGE bot is so careless in its plagiarism mashups that it also gives incorrect medical advice that has been drawn from a variety of sources. For example, I asked: “do I need a colonoscopy?” and it gave me the following answer:



(Image credit: Tom's Hardware)

I highlighted the text in blue because it is dangerously wrong. Google’s bot says that “the American Cancer Society recommends that men and women should be screened for colorectal cancer starting at age 50.” However, the American Cancer Society’s own website says that [screenings should start at age 45](https://www.cancer.org/cancer/types/colon-rectal-cancer/detection-diagnosis-staging/acs-recommendations.html), so this misleading “fact” probably came from elsewhere.

There’s also a bulleted list of “reasons to have a colonoscopy” that don’t include “routine screening,” hence it’s implying that you should only get the procedure if you have symptoms. The bulleted list is copied word-for-word from [an article](https://www.betterhealth.vic.gov.au/health/conditionsandtreatments/colonoscopy) on an Australian Government health site called BetterHealth. The article actually lists “screening and surveillance for colorectal cancer” as a reason, but Google’s bot decided not to copy that fact. 

Even if all the facts in the colonoscopy answer were clear and correct, they are not attributed to anyone. So why on earth should you trust them and whom do you blame when you follow this advice – for example, delaying your screening to age 50 – and something bad happens? By claiming content as its own, Google is acting as a publisher, which likely opens it up to lawsuits.

Though Google is telling the public that it wants to drive traffic to publishers, the SGE experience looks purpose-built to keep readers from leaving and going off to external sites, unless those external sites are ecomm vendors or advertisers. In some queries – “screenshot in windows” for example – there is a detailed answer but no related links at all. Nevermind that there are tons of articles that give you a lot more detail about how to take a screen shot.



(Image credit: Tom's Hardware)

If Google were to roll its SGE experience out of beta and make it the default, it would be detonating a 50-megaton bomb on the free and open web. Many publishers, who rely on Google referrals for the majority of their visits, would fold within a few months. Others would cut resources and retreat behind paywalls. Small businesses that rely on organic search placement to sell their products and services would have to either pay for advertising or, if they cannot afford it, close up shop. 

Eventually, even hobbyists who either run not-for-profit websites or post advice on forums would likely stop doing it. Who wants to write, even for fun, if your words are going to be stolen and no one is going to read your copy? Would you answer someone’s programming question on Stack Overflow if your contribution would just be reworded and spat out by Google, without ever mentioning your name or the post itself?

Not an AI Issue: An Anti-Competitive Issue
------------------------------------------

This isn’t a case of artificial intelligence outsmarting human writers or providing a better experience. In fact, the method of publishing is incidental to the problem. If it rolls the current SGE experience out, Google would be leveraging its monopoly position to push its own content over and above everyone else’s. The company could hire an army of unskilled writers to copy and paste content from third-party websites, sometimes rewording it, instead of using an AI. The outcome would be the same.

There’s no doubt that Google’s AI will get better, but get better at what exactly? It will likely do a better job of rephrasing content so that it’s harder to find the original source it copied from. It will do a better job of offering information that’s up-to-date and logically consistent with itself. However, by just grabbing other peoples’ ideas and not citing the source, there’s no authority behind anything it says.

The end result of Google SGE going live as the default search experience would be a weaker, more siloed Internet, but likely a wealthier Google. The company would increase its time-on-site, ad revenue and ecommerce referrals. It would also please investors, who want to see it compete with OpenAI and Bing. Some readers may grouse about the quality of the information, which can be outdated, false or word-for-word plagiarized, but taking up the entire first screen of results will be enough for Google to grab a huge percentage – if not the majority – of its current outbound clicks.

Many people I have talked to about and shown Google SGE can’t believe that the company would roll such a dangerous, poor-quality and web-breaking experience out to everyone. We can hope that the final product won’t take up as much screen real estate as what we’re seeing today. But Google is already making this the daily search experience for anyone who, like me, signs up for the beta. And it has every economic incentive to make this the new default experience for 91 percent of the web’s searches.

What Publishers Can Do, What Users Can Do
-----------------------------------------

Anyone who publishes on the web and needs people to actually read their work is in a precarious position, because of Google’s SGE. Almost every publication desperately needs to keep getting referrals from Google, so they can’t opt out of being indexed and having their data scraped. But if Google makes SGE the default search experience, the amount of Google referrals may fall so sharply that they can’t keep the lights on. 

Bing took only a few months to go from having its AI Chat in a limited beta to it being available to everyone. If Google follows a similar timeline, it could go from being a search engine to a zero-click, plagiarism engine by this fall.

Publishers and publishing associations are still grappling with what AI plagiarism could do to their businesses. The News / Media Alliance, an industry group that represents magazines and newspapers, published a [set of AI principles](https://www.newsmediaalliance.org/ai-principles/) that states “The unlicensed use of content created by our companies and journalists by GAI systems is an intellectual property infringement: GAI systems are using proprietary content without permission.”

Getty Images is [suing Stability AI](https://www.reuters.com/legal/getty-images-lawsuit-says-stability-ai-misused-photos-train-ai-2023-02-06/) to prevent the company from using its copyrighted images in training data. The image library has even asked a UK court to [block sales of the AI system](http://reuters.com/technology/getty-asks-london-court-stop-uk-sales-stability-ai-system-2023-06-01/) in that country. IAC Media Chairman Barry Diller has advocated for media companies to [sue AI vendors](https://fortune.com/2023/04/12/barry-diller-media-news-companies-a-i-sue-litigation/) over the unauthorized use of training data.

Will publishers sue Google over what it’s doing with SGE? There’s an argument that the word-for-word copying of information from websites without permission is a form of copyright infringement, even if the source was cited. However, we haven’t seen this litigated in court yet. And many companies, needing whatever traffic they will still get from Google, would want to avoid getting on the company’s bad side.

Companies could band together, through trade associations, to demand that Google respect intellectual property and not take actions that would destroy the open web as we know it. Readers can help by either scrolling past the company’s SGE to click on organic results or switching to a different search engine. Bing has shown a better way to incorporate AI, making its chatbot the non-default option and citing every piece of information it uses with a specific link back (the links aren’t very prominent, however). 

In the end, if Google follows through with its current iteration of SGE, it will damage the quality of its own service. The content that the bot trains on would get worse and worse as more quality publishers left the open web. Eventually, users would start looking for a service that provides better answers. But by that time, the damage done to the entire web information ecosystem could be irreparable.

_Note: As with all of our op-eds, the opinions expressed here belong to the writer alone and not Tom's Hardware as a team. _",Plagiarism Engine: Google’s Content-Swiping AI Could Break the Internet,https://www.tomshardware.com/news/google-sge-break-internet
"[""Federal Trade Commission""]",2023-07-24,2023-07-24,2023-05-22,2023-07-24,https://www.ftc.gov/themes/custom/ftc_uswds/img/ftc_social_share_default_en.jpg,,,en,,ftc.gov,"[""Khoa Lam""]","The Federal Trade Commission has obtained an order against education technology provider Edmodo for collecting personal data from children without obtaining their parent’s consent and using that data for advertising, in violation of the Children’s Online Privacy Protection Act Rule (COPPA Rule), and for unlawfully outsourcing its COPPA compliance responsibilities to schools. 

Under the [proposed order](/system/files/ftc_gov/pdf/2023129edmodojointmotionorder.pdf), filed by the Department of Justice on behalf of the FTC, Edmodo, Inc. will be prohibited from requiring students to hand over more personal data than is necessary in order to participate in an online educational activity. This is a first for an FTC order and is in line with a [policy statement the FTC issued in May 2022](/news-events/news/press-releases/2022/05/ftc-crack-down-companies-illegally-surveil-children-learning-online) that warned education technology companies about forcing parents and schools to provide personal data about children in order to participate in online education. During the course of the FTC’s investigation, Edmodo suspended operations in the United States. The order, if approved by the court, will bind the company, including if it resumes U.S. operations.

“This order makes clear that ed tech providers cannot outsource compliance responsibilities to schools, or force students to choose between their privacy and education,” said Samuel Levine, Director of the FTC’s Bureau of Consumer Protection. “Other ed tech providers should carefully examine their practices to ensure they’re not compromising students’ privacy.”

In a [complaint](/system/files/ftc_gov/pdf/edmodocomplaintfiled.pdf), also filed by DOJ, the FTC says Edmodo violated the COPPA Rule by failing to provide information about the company’s data collection practices to schools and teachers, and failing to obtain verifiable parental consent. The COPPA Rule requires online services and websites directed to children under 13 to notify parents about the personal information they collect and obtain verifiable parental consent for the collection and use of that information.

Until approximately September 2022, California-based Edmodo offered an online platform and mobile app with virtual class spaces to host discussions, share materials and other online resources for teachers and schools in the United States via a free and subscription-based service. The company collected personal information about students including their name, email address, date of birth and phone number as well as persistent identifiers, which it used to provide ads.

Under the COPPA Rule, schools can authorize collection of children’s personal information on behalf of parents. But a website operator must provide notice to the school of the operator’s collection, use and disclosure practices, and the school can only authorize collection and use of personal information for an educational purpose.

Edmodo required schools and teachers to authorize data collection on behalf of parents or to notify parents about Edmodo’s data collection practices and obtain their consent to that collection. Edmodo, however, failed to provide schools and teachers with the information they would need to comply in either scenario as required by the COPPA Rule, according to the complaint. For example, during the signup process for Edmodo’s free service, Edmodo provided minimal information about the COPPA Rule to teachers—providing only a link to the company’s terms of service and privacy policy, which teachers were not required to review before signing up for the company’s service.

Those teachers and schools that did read Edmodo’s terms of service were falsely told that they were “solely” responsible for complying with the COPPA Rule. The terms of service also failed to adequately disclose what personal information the company actually collects or indicate how schools or teachers should go about obtaining parental consent. These failures led to the illegal collection of personal information from children, according to the complaint.

In addition, Edmodo could not rely on schools to authorize collection on behalf of parents because the company used the personal information it collected from children for a non-educational purpose—to serve advertising. For such commercial uses, the COPPA Rule required Edmodo to obtain consent directly from parents. 

Edmodo also violated the COPPA Rule by retaining personal information indefinitely until at least 2020 when it put in place a policy to delete the data after two years, according to the complaint. COPPA prohibits retaining personal information about children for longer than is reasonably necessary to fulfill the purpose for which it was collected.

In addition to violating the COPPA Rule, the FTC says Edmodo violated the FTC Act’s prohibition on unfair practices by relying on schools to obtain verifiable parental consent. Specifically, the FTC says that Edmodo outsourced its COPPA compliance responsibilities to schools and teachers while providing confusing and inaccurate information about obtaining consent. This is the first time the FTC has alleged an unfair trade practice in the context of an operator’s interaction with schools.

Proposed Order
--------------

The proposed order with Edmodo includes a $6 million monetary penalty, which will be suspended due to the company’s inability to pay. Other order provisions, which will provide protections for children’s data should Edmodo resume operations in the United States, include:

*   prohibiting Edmodo from conditioning a child’s participation in an activity on the child disclosing more information than is reasonably necessary to participate in such activity;
*   requiring the company to complete several requirements before obtaining school authorization to collect information from a child;
*   prohibiting the company from using children’s information for non-educational purposes such as advertising or building user profiles;
*   banning the company from using schools as intermediaries in the parental consent process;
*   requiring the company to implement and adhere to a retention schedule that details what information it collects, what the data is used for and a time frame for deleting it; and
*   requiring Edmodo to delete models or algorithms developed using personal information collected from children without verifiable parental consent or school authorization.

The Commission voted 3-0 to refer the civil penalty complaint and proposed federal order to the Department of Justice. The DOJ filed the complaint and stipulated order in the U.S. District Court for the Northern District of California.

**NOTE:** The Commission authorizes the filing of a complaint when it has “reason to believe” that the named defendant is violating or is about to violate the law and it appears to the Commission that a proceeding is in the public interest. Stipulated orders have the force of law when approved and signed by the District Court judge.

The lead FTC attorneys on this matter are Gorana Neskovic and Peder Magee from the FTC’s Bureau of Consumer protection.",FTC Says Ed Tech Provider Edmodo Unlawfully Used Children’s Personal Information for Advertising and Outsourced Compliance to School Districts,https://www.ftc.gov/news-events/news/press-releases/2023/05/ftc-says-ed-tech-provider-edmodo-unlawfully-used-childrens-personal-information-advertising
"[""Nico Grant"",""Kashmir Hill""]",2023-07-24,2023-07-24,2023-05-22,2023-07-24,https://static01.nyt.com/images/2023/05/16/multimedia/g-0-promo/g-0-promo-facebookJumbo.jpg,,,en,,nytimes.com,"[""Khoa Lam""]","Eight years after a controversy over Black people being mislabeled as gorillas by image analysis software — and despite big advances in computer vision — tech giants still fear repeating the mistake.

When Google released its stand-alone Photos app in May 2015, people were wowed by what it could do: analyze images to label the people, places and things in them, an astounding consumer offering at the time. But a couple of months after the release, a software developer, Jacky Alciné, discovered that Google had labeled photos of him and a friend, who are both Black, as “gorillas,” a term that is particularly offensive because it echoes centuries of racist tropes.

In the ensuing controversy, Google prevented its software from categorizing anything in Photos as gorillas, and it vowed to fix the problem. Eight years later, with significant advances in artificial intelligence, we tested whether Google had resolved the issue, and we looked at comparable tools from its competitors: Apple, Amazon and Microsoft.  
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

Photo apps made by Apple, Google, Amazon and Microsoft rely on artificial intelligence to allow us to search for particular items, and pinpoint specific memories, in our increasingly large photo collections. Want to find your day at the zoo out of 8,000 images? Ask the app. So to test the search function, we curated **44 images** featuring people, animals and everyday objects.

We started with Google Photos. When we searched our collection for **cats** and **kangaroos,** we got images that matched our queries. The app performed well in recognizing most other animals.

But when we looked for **gorillas,** Google Photos failed to find any images. We widened our search to **baboons, chimpanzees, orangutans** and **monkeys,** and it still failed even though there were images of all of these primates in our collection.

We then looked at Google’s competitors. We discovered Apple Photos had the same issue: It could accurately find photos of particular animals, except for most primates. We did get results for **gorilla,** but only when the text appeared in a photo, such as an image of Gorilla Tape.

The photo search in Microsoft OneDrive drew a blank for every animal we tried. Amazon Photos showed results for all searches, but it was over-inclusive. When we searched for **gorillas,** the app showed a menagerie of primates, and repeated that pattern for other animals.

undefined

There was one member of the primate family that Google and Apple were able to recognize — lemurs, the permanently startled-looking, long-tailed animals that share opposable thumbs with humans, but are more distantly related than are apes.

Google’s and Apple’s tools were clearly the most sophisticated when it came to image analysis.

Yet Google, whose Android software underpins most of the world’s smartphones, has made the decision to turn off the ability to visually search for primates for fear of making an offensive mistake and labeling a person as an animal. And Apple, with technology that performed similarly to Google’s in our test, appeared to disable the ability to look for monkeys and apes as well.

Consumers may not need to frequently perform such a search — though in 2019, an iPhone user complained on Apple’s customer support forum that the software “[can’t find monkeys in photos on my device](https://discussions.apple.com/thread/250713142).” But the issue raises larger questions about other unfixed, or unfixable, flaws lurking in services that rely on computer vision — a technology that interprets visual images — as well as other products powered by A.I.

Mr. Alciné was dismayed to learn that Google has still not fully solved the problem and said society puts too much trust in technology.

“I’m going to forever have no faith in this A.I.,” he said.

Computer vision products are now used for tasks as mundane as sending an alert when there is a package on the doorstep, and as weighty as navigating cars and finding perpetrators in law enforcement investigations.

Errors can reflect racist attitudes among those encoding the data. In the gorilla incident, two former Google employees who worked on this technology said the problem was that the company had not put enough photos of Black people in the image collection that it used to train its A.I. system. As a result, the technology was not familiar enough with darker-skinned people and confused them for gorillas.

As artificial intelligence becomes more embedded in our lives, it is eliciting fears of unintended consequences. Although computer vision products and A.I. chatbots like ChatGPT are different, both depend on underlying reams of data that train the software, and both can misfire because of flaws in the data or biases incorporated into their code.

Microsoft recently [limited users’ ability](https://www.nytimes.com/2023/02/16/technology/microsoft-bing-chatbot-limits.html) to interact with a chatbot built into its search engine, Bing, after it instigated [inappropriate conversations](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html).

Microsoft’s decision, like Google’s choice to prevent its algorithm from identifying gorillas altogether, illustrates a common industry approach — to wall off technology features that malfunction rather than fixing them.

“Solving these issues is important,” said Vicente Ordóñez, a professor at Rice University who studies computer vision. “How can we trust this software for other scenarios?”

Michael Marconi, a Google spokesman, said Google had prevented its photo app from labeling anything as a monkey or ape because it decided the benefit “does not outweigh the risk of harm.”

Apple declined to comment on users’ inability to search for most primates on its app.

Representatives from Amazon and Microsoft said the companies were always seeking to improve their products.

When Google was developing its photo app, which was released eight years ago, it collected a large amount of images to train the A.I. system to identify people, animals and objects.

Its significant oversight — that there were not enough photos of Black people in its training data — caused the app to later malfunction, two former Google employees said. The company failed to uncover the “gorilla” problem back then because it had not asked enough employees to test the feature before its public debut, the former employees said.

Google profusely apologized for the gorillas incident, but it was one of a number of episodes in the wider tech industry that have led to accusations of bias.

Other products that have been criticized include [HP’s facial-tracking webcams](https://www.reuters.com/article/urnidgns852573c40069388000257693007c9b22/hp-our-webcams-arent-racist-idUS97608933020091222), which could not detect some people with dark skin, and the [Apple Watch,](https://splinternews.com/will-the-apple-watchs-coolest-feature-work-for-people-o-1793846147) which, according [to a lawsuit](https://www.usatoday.com/story/tech/2022/12/27/apple-watch-blood-oxygen-oximeter-dark-skin-lawsuit/10955942002/), failed to accurately read blood oxygen levels across skin colors. The lapses suggested that tech products were not being designed for people with darker skin. (Apple pointed [to a paper](https://www.apple.com/healthcare/docs/site/Blood_Oxygen_app_on_Apple_Watch_October_2022.pdf) from 2022 that detailed its efforts to test its blood oxygen app on a “wide range of skin types and tones.”)

Years after the Google Photos error, the company encountered a similar problem with its Nest home-security camera during internal testing, according to a person familiar with the incident who worked at Google at the time. The Nest camera, which used A.I. to determine whether someone on a property was familiar or unfamiliar, mistook some Black people for animals. Google rushed to fix the problem before users had access to the product, the person said.

However, Nest customers continue to complain on the company’s forums about other flaws. In 2021, a customer received alerts that his mother was ringing the doorbell but found his mother-in-law instead on the other side of the door. When users complained that the system was mixing up faces they had marked as “familiar,” a customer support representative in the forum advised them to delete all of their labels and start over.

Mr. Marconi, the Google spokesman, said that “our goal is to prevent these types of mistakes from ever happening.” He added that the company had improved its technology “by partnering with experts and diversifying our image datasets.”

In 2019, Google tried to improve a facial-recognition feature for Android smartphones by increasing the number of people with dark skin in its data set. But the contractors whom Google had hired to collect facial scans [reportedly](https://www.nytimes.com/2019/10/04/technology/google-facial-recognition-atlanta-homeless.html) resorted to a troubling tactic to compensate for that dearth of diverse data: They targeted homeless people and students. Google executives called the incident “very disturbing” at the time.

While Google worked behind the scenes to improve the technology, it never allowed users to judge those efforts.

Margaret Mitchell, a researcher and co-founder of Google’s Ethical AI group, joined the company after the gorilla incident and collaborated with the Photos team. She said in a recent interview that she was a proponent of Google’s decision to remove “the gorillas label, at least for a while.”

“You have to think about how often someone needs to label a gorilla versus perpetuating harmful stereotypes,” Dr. Mitchell said. “The benefits don’t outweigh the potential harms of doing it wrong.”

Dr. Ordóñez, the professor, speculated that Google and Apple could now be capable of distinguishing primates from humans, but that they didn’t want to enable the feature given the possible reputational risk if it misfired again.

Google has since released a more powerful image analysis product, Google Lens, a tool to search the web with photos rather than text. [Wired](https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/) discovered in 2018 that the tool was also unable to identify a gorilla.



When we showed Lens a photo of a dog, it was able to suggest its likely breed.

But when we showed it a gorilla, a chimpanzee, a baboon, and an orangutan, Lens seemed to be stumped, refusing to label what was in the image and surfacing only “visual matches” — photos it deemed similar to the original picture.

For gorillas, it showed photos of other gorillas, suggesting that the technology recognizes the animal but that the

company is afraid of labeling it.



When we showed Lens a photo of a dog, it was able to suggest its likely breed.

But when we showed it a gorilla, a chimpanzee, a baboon, and an orangutan, Lens seemed to be stumped, refusing to label what was in the image and surfacing only “visual matches” — photos it deemed similar to the original picture.

For gorillas, it showed photos of other gorillas, suggesting that the technology recognizes the animal but that the company is afraid of labeling it.

undefined

These systems are never foolproof, said Dr. Mitchell, who is no longer working at Google. Because billions of people use Google’s services, even rare glitches that happen to only one person out of a billion users will surface.

“It only takes one mistake to have massive social ramifications,” she said, referring to it as “the poisoned needle in a haystack.”",Google’s Photo App Still Can’t Find Gorillas. And Neither Can Apple’s,https://www.nytimes.com/2023/05/22/technology/ai-photo-labels-google-apple.html
"[""Nino Bucci"",""Christopher Knaus""]",2023-07-24,2023-07-24,2023-05-12,2023-07-24,https://i.guim.co.uk/img/media/9b98427a63c5a7e43cd10797b05627bd25756889/0_89_3898_2340/master/3898.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=316b7d26ef5052da117e2ee21e6b39e8,,,en,,theguardian.com,"[""Khoa Lam""]","A tool designed to predict future crime in terrorist offenders considered them at greater risk of offending if they were autistic despite having no empirical basis to do so, an independent report has found.

The report into the Vera-2R tool, which was released to Guardian Australia and others under freedom of information laws, found a lack of evidence underpinning the instrument had “potentially serious implications for \[its\] validity and reliability” and found it was “extremely poor” at predicting risk.

It also found that autism spectrum disorders, as well as non-compliance with conditions or supervision, were included as risk factors in the tool, despite a lack of empirical evidence.

The most comprehensive systematic review that has been conducted on the drivers of radicalisation and terrorist behaviour and violence found not a single piece of empirical evidence that supported the inclusion of those two factors, the report found.

The release of the critical report came after a series of Guardian Australia stories which exposed the federal government’s continued use of the tool despite criticisms. The federal government did not disclose the report [to the lawyers of convicted offenders](https://www.theguardian.com/australia-news/2023/apr/28/lawyers-outraged-by-government-failure-to-disclose-terrorism-prediction-tools-serious-problems), its own experts, or the NSW government, which uses the tool to justify harsh post-sentence orders for offenders, including [ongoing detention](https://www.theguardian.com/law/2023/apr/04/law-council-joins-calls-to-abolish-australias-powers-to-detain-terrorist-offenders-to-prevent-future-crimes).

The report, titled “Testing the Reliability, Validity and Equity of Terrorism Risk Assessment Instruments”, was completed by Australian National University academics Dr Emily Corner and Dr Helen Taylor for the Department of Home Affairs and runs to more than 270 pages.

The government received the report, known as the Corner report, in May 2020.

It was released under FoI with minimal redactions, despite the federal government previously claiming it could not be released because of national security.

“The lack of evidence underpinning both instruments has potentially serious implications for their validity and reliability,” the report found.

“Without a strong theoretical and empirical basis for factor inclusion, it is not reasonable to anticipate that the instruments are able to predict their specified risk with anything other than chance.

“If an instrument with a weak evidence base is employed as a predictive instrument by practitioners, it is not possible to determine if individuals who pass through assessment processes would ever be suitable for the management plan as determined by the risk decision outcome made on the instrument.”

Guardian Australia has previously reported the tool was [used 14 times by the federal government](https://www.theguardian.com/australia-news/2023/apr/08/coalition-warned-of-problems-with-tool-to-predict-future-terrorist-but-continued-to-use-it-on-offenders) after they received the report, and the NSW government also continued to use it to assess offenders, including some without convictions for terrorism.

Corner and Taylor also assessed the Radar tool, a classified instrument which is used less frequently to justify post-sentence orders than Vera-2R, and said their report was the first piece of research to be performed on these instruments.

The report made four recommendations, including that another evaluation is conducted, that the authors of both instruments “enact far more thorough evaluations of the wider theoretical and empirical literature to help develop risk factors that accurately reflect behavioural trajectories towards radicalisation and terrorist violence”, and that the risk specification of each instrument should be refined.

[skip past newsletter promotion](https://www.theguardian.com/australia-news/2023/may/12/australian-terrorism-prediction-tool-considered-autism-a-sign-of-criminality-despite-lack-of-evidence#EmailSignup-skip-link-16)

Sign up to Guardian Australia's Afternoon Update

Our Australian afternoon update email breaks down the key national and international stories of the day and why they matter

**Privacy Notice:** Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our [Privacy Policy](https://www.theguardian.com/help/privacy-policy). We use Google reCaptcha to protect our website and the Google [Privacy Policy](https://policies.google.com/privacy) and [Terms of Service](https://policies.google.com/terms) apply.

after newsletter promotion

It found that the correct instrument, implemented correctly, could allow risk assessment that ensured those at highest risk of offending received the most intensive interventions, while those presenting as lower-risk were protected from too much intervention and therefore avoided “the disruption of circumstances that were making them low risk in the first place”.

But it found there was little basis for the claims made by the developers of the Vera-2R of its strong reliability and validity.

It said it was “concerning” that almost 60% of the cited evidence base for factor development of the tool was not empirical, and less than half of the works cited in the Vera-2R accurately reflected what was in the recorded texts.

The test can only be used by people who complete training accredited to the author, with the report noting that “there is no information within any documentation as to the time or costs of this training program, but it is believed to be a multiple day program with substantial costs attached”.

The federal and NSW governments both say they are considering the Corner report findings, but this week’s federal budget shows the Labor government is committed to continuing to fund its regime of post-sentence orders.

The budget gave $130.1m over two years to the high-risk terrorist offenders scheme.

A NSW communities and justice department spokesperson said the Vera-2R tool was just one of a number of tools used to assess risk.

“The Department of Communities and Justice is currently considering the Corner report in collaboration with other stakeholders,” the spokesperson said.",Australian terrorism prediction tool considered autism a sign of criminality despite lack of evidence,https://www.theguardian.com/australia-news/2023/may/12/australian-terrorism-prediction-tool-considered-autism-a-sign-of-criminality-despite-lack-of-evidence
"[""Jess Weatherbed""]",2023-07-24,2023-07-24,2023-05-15,2023-07-24,https://cdn.vox-cdn.com/thumbor/8KYMU6i45uSgH2BOZjCAUQ6U-VE=/0x0:1920x1200/1200x628/filters:focal(953x629:954x630)/cdn.vox-cdn.com/uploads/chorus_asset/file/24658583/House_of_Earth_and_Blood_Adobe_Stock.jpg,,,en,,theverge.com,"[""Khoa Lam""]","A prominent fantasy novel features a cover apparently generated with artificial intelligence, [sparking complaints](https://www.reddit.com/r/SarahJMaas/comments/138w0i1/crescent_city_ai_generated_covers/) from artists and book enthusiasts. Earlier this month, readers noted that the back of the UK edition of Sarah J. Maas’ _House of Earth and Blood_ [credits Adobe Stock](https://blackwells.co.uk/bookshop/product/9781526663559) for the illustration of a wolf on its cover. The illustration matches an image created by user [Aperture Vintage](https://go.redirectingat.com/?xs=1&id=1025X1701640&url=https%3A%2F%2Fstock.adobe.com%2Fuk%2Fcontributor%2F203047531%2Faperture-vintage%3Fload_type%3Dauthor%26prev_url%3Ddetail%26asset_id%3D550401570) and marked as AI-generated on Adobe’s site. The move has led to criticism of both Maas and Bloomsbury Publishing, one of the world’s leading independent publishing houses.

While stock image services like Getty Images have prohibited AI-generated illustrations to avoid [copyright disputes](https://www.theverge.com/2023/2/6/23587393/ai-art-copyright-lawsuit-getty-images-stable-diffusion), Adobe has [notably welcomed](https://helpx.adobe.com/uk/stock/contributor/help/generative-ai-content.html) AI onto its Adobe Stock platform under specific criteria. Such content must be clearly labeled as AI-generated, and contributors must review the terms of any generative AI tools they use to create the images to ensure they have “all the necessary rights” to license them for commercial use. The rules are meant to avoid legal headaches. But the field of AI copyright is muddy, untested territory, and Adobe’s rules don’t clearly address all the issues it raises.

Bloomsbury rose to fame in the UK after it began publishing J.K. Rowling’s _Harry Potter_ franchise in 1997. Maas is currently one of Bloomsbury’s top authors, best known for her young adult fantasy series like _Throne of Glass_ (2012), _A Court of Thorns and Roses_ (2015), and _Crescent City_ (2020) — the franchise that _House of Earth and Blood_ is part of. She has sold over 12 million copies of her books, many of which have made it to the [_New York Times_ bestsellers list](https://www.nytimes.com/books/best-sellers/2021/03/07/), and a televised adaptation of _A Court of Thorns and Roses_ is currently [in development at Hulu](https://deadline.com/2021/03/a-court-of-thorns-and-roses-series-fantasy-books-hulu-ron-moore-sarah-j-maas-1234722918/). The publisher, Maas, Adobe, and Aperture Vintage did not immediately respond to requests for comment from _The Verge_.

Bloomsbury’s cover has exacerbated human artists’ concerns that publishers could replace them with text-to-image generators like Midjourney and Stable Diffusion. “Bloomsbury is one of the major publishing houses,” said [freelance artist Kala Elizabeth](https://twitter.com/kalaelizabeth/status/1657065874711543808) on Twitter. “They CAN afford to hire real illustrators instead of purchasing Adobe stock, which is where this AI content is from.” Other publishers have attracted similar scrutiny for using AI-generated artwork on covers. [Tor Books issued an apology](https://gizmodo.com/tor-book-ai-art-cover-christopher-paolini-fractalverse-1849904058) in December last year, claiming it was unaware that the cover image selected for Christopher Paolini’s _Fractal Noise_ novel was created by AI. While Maas hasn’t acknowledged the image’s provenance, and it’s not clear what — if any — involvement she had in the process, she’s praised the design of the cover on her [Instagram page](https://www.instagram.com/p/Co7pZ39LHGF/?hl=en).

Adobe launched its own [Firefly AI image generator](https://www.theverge.com/2023/3/21/23648315/adobe-firefly-ai-image-generator-announced) that it says is trained only on content that’s licensed or out of copyright. But those assurances don’t apply to images found in Adobe Stock, raising questions about whether copyrighted work was used to train the image generator that produced it. Many AI systems are built around datasets containing artwork scraped from the internet without the consent of the original artist, sometimes even [displaying signatures](https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit) from the original artwork within a generated final image. This has led [some artists](https://www.theverge.com/2022/12/23/23523864/artstation-removing-anti-ai-protest-artwork-censorship) to believe that [all AI-generated art is unethical](https://www.theverge.com/2023/1/16/23557098/generative-ai-art-copyright-legal-lawsuit-stable-diffusion-midjourney-deviantart) as it can profit from the work of human artists. It isn’t clear whether Adobe is assessing images to ensure they comply with its rules about copyright or whether it’s placing the legal responsibilities on creators.",A bestselling fantasy novel is using AI-generated cover art,https://www.theverge.com/2023/5/15/23724102/sarah-j-maas-ai-generated-book-cover-bloomsbury-house-of-earth-and-blood
"[""James Vincent""]",2023-07-24,2023-07-24,2023-05-02,2023-07-24,https://cdn.vox-cdn.com/thumbor/WgCVKvKeU4UBnz8GJYvIkkYyYyc=/0x0:5000x3120/1200x628/filters:focal(2500x1560:2501x1561)/cdn.vox-cdn.com/uploads/chorus_asset/file/24626885/537799082.jpg,,,en,,theverge.com,"[""Khoa Lam""]","AI chatbots are being used to generate news stories and blog posts for online content farms in the hopes of attracting a trickle of ad revenue from the stray clicks of web users.

Experts have been warning for years that such AI-generated content farms will soon become commonplace, but the wider availability of tools like OpenAI’s ChatGPT has now made these warnings a reality. NewsGuard, a for-profit organization that rates the trustworthiness of news sites, highlighted the problem in a [recent report](https://www.newsguardtech.com/special-reports/newsbots-ai-generated-news-websites-proliferating/) identifying 49 sites “that appear to be almost entirely written by artificial intelligence software.”

> The websites, which often fail to disclose ownership or control, produce a high volume of content related to a variety of topics, including politics, health, entertainment, finance, and technology. Some publish hundreds of articles a day. Some of the content advances false narratives. Nearly all of the content features bland language and repetitive phrases, hallmarks of artificial intelligence.

The sites identified by the organization often have generic names (like _Biz Breaking News_ and _Market News Reports_) and are stuffed with programmatic advertising that’s bought and sold automatically. They attribute news stories to generic or fake authors, and much of the content appears to be summaries or re-writes of stories from established sites like _CNN_.

Most of the sites are not spreading misinformation, said NewsGuard, but some publish blatant falsehoods. For example, in early April, a content farm named CelebritiesDeaths.com posted a story [claiming that Joe Biden had died](https://web.archive.org/web/20230409093456/https://celebritiesdeaths.com/biden-dead-harris-acting-president-address-9am-et/).

This Biden story might briefly fool a reader, though is soon revealed to be a fake. The second paragraph contains an error message from the chatbot that was asked to create the text and was evidently copy and pasted into the website without any oversight. “I’m sorry, I cannot complete this prompt as it goes against OpenAI’s use case policy on generating misleading content,” says the story. “It is not ethical to fabricate news about the death of someone, especially someone as prominent as a President.”

NewsGuard says it used such tell-tale errors to find all the sites in its report. As _The Verge_ has previously reported, [searching for phrases like “As an AI language model”](https://www.theverge.com/2023/4/25/23697218/ai-generated-spam-fake-user-reviews-as-an-ai-language-model) often reveals where chatbots are being used to generate fake reviews and other cheap text content. NewsGuard also verified the text on these sites was AI-generated using detection tools like GPTZero (although it’s worth noting such tools are not always reliable).

Noah Giansiracusa, an associate professor of data science who’s written about fake news, [told _Bloomberg_](https://www.bloomberg.com/news/articles/2023-05-01/ai-chatbots-have-been-used-to-create-dozens-of-news-content-farms?sref=ExbtjcSG) that the creators of such sites were experimenting “to find what’s effective” and would continue to spin up content farms given the cheap costs of production. “Before, it was a low-paid scheme. But at least it wasn’t free,” Giansiracusa told the outlet.

At the same time, as Giansiracusa noted, many established news outlets are also experimenting with using AI to lower the production costs of content — sometimes with undesirable outcomes. When _CNET_ started using AI to help write posts, a review of the system’s output found [errors in more than half the published stories](https://www.theverge.com/2023/1/25/23571082/cnet-ai-written-stories-errors-corrections-red-ventures). The pressure to use AI is increasing at a time when online news is facing a wave of layoffs and shut-downs.

You can read the full report from NewsGuard [here](https://www.newsguardtech.com/special-reports/newsbots-ai-generated-news-websites-proliferating/).",AI is being used to generate whole spam sites,https://www.theverge.com/2023/5/2/23707788/ai-spam-content-farm-misinformation-reports-newsguard
"[""Will Oremus""]",2023-07-24,2023-07-24,2023-05-05,2023-07-24,,,,en,,washingtonpost.com,"[""Khoa Lam""]","Chris Cowell, a Portland, Ore.-based software developer, spent more than a year writing a technical how-to book. Three weeks before it was released, another book on the same topic, with the same title, appeared on Amazon.",He wrote a book on a rare subject. Then a ChatGPT replica appeared on Amazon.,https://www.washingtonpost.com/technology/2023/05/05/ai-spam-websites-books-chatgpt
"[""Daniel Johnson""]",2023-07-24,2023-07-24,2023-07-20,2023-07-24,https://a9p9n2x2.stackpathcdn.com/wp-content/blogs.dir/1/files/2023/07/GettyImages-1299513271-e1689787410868.jpg,,,en,,blackenterprise.com,"[""Khoa Lam""]","[Daniel Johnson](https://www.blackenterprise.com/author/danieljohnson/)July 20, 2023July 20, 2023611

[0](#)[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.blackenterprise.com%2Fkhan-academys-harriet-tubman-ai%2F)[](https://twitter.com/intent/tweet?text=Khan%20Academy%20Is%20On%20The%20Wrong%20Side%20Of%20History%20And%20Intelligence%20With%20Its%20Harriet%20Tubman%20AI%20Avatar%20-%20https://www.blackenterprise.com/khan-academys-harriet-tubman-ai/)[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.blackenterprise.com%2Fkhan-academys-harriet-tubman-ai%2F&title=Khan%20Academy%20Is%20On%20The%20Wrong%20Side%20Of%20History%20And%20Intelligence%20With%20Its%20Harriet%20Tubman%20AI%20Avatar)[](mailto:?subject=Khan%20Academy%20Is%20On%20The%20Wrong%20Side%20Of%20History%20And%20Intelligence%20With%20Its%20Harriet%20Tubman%20AI%20Avatar&BODY=https://www.blackenterprise.com/khan-academys-harriet-tubman-ai/)



Activist and abolitionist Harriet Tubman, in 1895. (Smith Collection/Gado/Getty Images)

[Khan Academy](https://www.khanacademy.org/khan-labs#whoIsKhanAcademy) bills its artificial intelligence tutors as “the future of learning” on its website, but the truth is a little more complicated. What the site does not state upfront is that its service allows learners to select different historical figures such as Genghis Khan, Montezuma, Abigail Adams, and Harriet Tubman. The service is currently not available to all; it is restricted to a few school districts as well as volunteer testers of the product.

Similar to ChatGPT, avatars pull from data available on the internet to create a repository of words in the “vocabulary” of the bot that a user is talking to.

_The_ _Washington Post_[tested the limits](https://www.washingtonpost.com/history/interactive/2023/harriet-tubman-articial-intelligence-khan-academy/) of this technology, specifically the avatar of Harriet Tubman, to see if the AI would mimic Tubman’s speech pattern and spirit or if came off as an offensive impression or a regurgitation of Wikipedia information. 

According to the article, the tool is designed to assist educators in fostering students’ curiosity of historical figures, but there are limits in how the bot is programmed, resulting in avatars that do not accurately portray the figures they are supposed to represent. 

These AI interviews immediately raised questions, not just of the ethics in the nascent field of artificial intelligence, but of the ethics in even conducting such an “interview” in the interest of journalism. Many Black users on Twitter were horrified at the thought of digitally exhuming a venerated icon and ancestor in Harriet Tubman. These concerns seem to be located in the working knowledge that the creators of these apps and bots are not interested in fidelity to the spirits of the dead, because they don’t seem to care much about the living Black people they continually fail to do right by.

Even _The_ _Washington Post_ acknowledges that the bot fails its basic fact-checks, and Khan Academy stresses that the bot is not intended to function as a historical record of events. Why introduce such a technology if it cannot be trusted to even impersonate an up-to-date “version” of historical figures?

> What is wrong with y’all? [pic.twitter.com/0RXNDKeVf0](https://t.co/0RXNDKeVf0)
> 
> — CiCi Adams (@CiCiAdams\_) [July 18, 2023](https://twitter.com/CiCiAdams_/status/1681310375319584771?ref_src=twsrc%5Etfw)

[UNESCO](https://www.unesco.org/en/artificial-intelligence/recommendation-ethics) sets out some basic tenets and recommendations for ethics in the field of artificial intelligence on its website. The organization created the first global standard for ethics in artificial intelligence, which was accepted by 193 countries around the world in 2021.

Their four pillars are Human Rights and Human Dignity, Living in Peace With an Emphasis on Creating a Just Society, Ensuring Diversity and Inclusion, and Environmentalism. Even a cursory glance at these pillars would find Khan Academy’s bot impersonating historical figures who can’t consent to have their likenesses and names used is in flagrant violation of ethics and, some would argue, moral guidelines.

If the dead have dignity, digging them up for what amounts to thought exercises represents a complete disregard for their wishes and a lack of thought about these tenets of ethics. In its discussion of fairness and nondiscrimination, UNESCO writes: “AI actors should promote social justice, fairness, and non-discrimination while taking an inclusive approach to ensure AI’s benefits are accessible to all.”

It sounds like Khan Academy needs to take these words to heart, because at present, it does not exactly seem like social justice, fairness, and accessibility are at the heart of this project. The reactions to this experiment on social media tell that story to the world.

**RELATED CONTENT**: [Redman Wants No Parts Of Artificial Intelligence Says, ‘Don’t Let Technology Ruin Hip-Hop’](https://www.blackenterprise.com/celebrity-news-redman-artificial-intelligence/)",Khan Academy Is On The Wrong Side Of History And Intelligence With Its Harriet Tubman AI Avatar,https://www.blackenterprise.com/khan-academys-harriet-tubman-ai/
"[""Shira Ovide""]",2023-07-24,2023-07-24,2023-07-11,2023-07-24,,,,en,,washingtonpost.com,"[""Khoa Lam""]","On his way to catch a flight, Sen. Jeff Merkley (D-Ore.) was asked to have his photo taken by a facial recognition machine at airport security.",You can say no to a TSA face scan. But even a senator had trouble.,https://www.washingtonpost.com/technology/2023/07/11/tsa-airport-security-facial-recognition/
"[""Associated Press""]",2023-07-27,2023-07-31,2023-07-12,2023-07-27,https://assets.apnews.com/31/ec/538fe67dcde24eb2675a9c2642aa/bc0a53a175974a2cbfe769dce4f64c58,,,en,,apnews.com,"[""Anonymous""]","SACRAMENTO, Calif. (AP) — A federal lawsuit alleges that health insurance giant Cigna used a computer algorithm to automatically reject hundreds of thousands of patient claims without examining them individually as required by California law.

The class-action lawsuit, filed Monday in federal court in Sacramento, says Cigna Corp. and Cigna Health and Life Insurance Co. rejected more than 300,000 payment claims in just two months last year.

The company used an algorithm called PXDX, shorthand for ''procedure-to-diagnosis,” to identify whether claims met certain requirements, spending an average of just 1.2 seconds on each review, according to the lawsuit. Huge batches of claims were then sent on to doctors who signed off on the denials, the lawsuit said.

“Relying on the PXDX system, Cigna’s doctors instantly reject claims on medical grounds without ever opening patient files, leaving thousands of patients effectively without coverage and with unexpected bills,” according to the lawsuit.

Ultimately, Cigna conducted an “illegal scheme to systematically, wrongfully and automatically” deny members claims to avoid paying for medical necessary procedures, the lawsuit contends.

Connecticut-based Cigna has 18 million U.S. members, including more than 2 million in California.

The lawsuit was filed on behalf of two Cigna members in Placer and San Diego counties who were forced to pay for tests after Cigna denied their claims.

The lawsuit accuses Cigna of violating California’s requirement that it conduct “thorough, fair, and objective” investigations of bills submitted for medical expenses. It seeks unspecified damages and a jury trial.

Cigna “utilizes the PXDX system because it knows it will not be held accountable for wrongful denials” because only a small fraction of policyholders appeal denied claims, according to the lawsuit.

In a statement, Cigna Healthcare said the lawsuit “appears highly questionable and seems to be based entirely on a poorly reported article that skewed the facts.”

The company says the process is used to speed up payments to physicians for common, relatively inexpensive procedures through an industry-standard review process similar to those used by other insurers for years.

“Cigna uses technology to verify that the codes on some of the most common, low-cost procedures are submitted correctly based on our publicly available coverage policies, and this is done to help expedite physician reimbursement,” the statement said. “The review takes place after patients have received treatment, so it does not result in any denials of care. If codes are submitted incorrectly, we provide clear guidance on resubmission and how to appeal.”",Cigna Health Giant Accused of Improperly Rejecting Thousands of Patient Claims Using an Algorithm,https://apnews.com/article/cigna-california-health-coverage-lawsuit-4543b47cd6057519a7e8dc6d90a61866
"[""Mirna Alsharif"",""Cristian Santana""]",2023-08-07,2023-08-07,2023-08-07,2023-08-07,"https://media-cldnry.s-nbcnews.com/image/upload/t_nbcnews-fp-1200-630,f_auto,q_auto:best/rockcms/2023-08/230806-porcha-woodruff-jm-1716-d060f4.jpg",,,en,,nbcnews.com,"[""Khoa Lam""]","A Detroit woman is suing the city and a police detective after she was falsely arrested because of facial recognition technology while she was eight months pregnant, according to court documents.

Porcha Woodruff, 32, was getting her two children ready for school on the morning of Feb. 16 when six police officers showed up at her doorstep and presented her with an arrest warrant alleging robbery and carjacking.

Woodruff initially believed the officers were joking given her visibly pregnant state. She was arrested.

""Ms. Woodruff later discovered that she was implicated as a suspect through a photo lineup shown to the victim of the robbery and carjacking, following an unreliable facial recognition match,"" court documents say.

The robbery victim told police that on Jan. 29 he met a woman whom he had sexual intercourse with. At some point in the day, they went to a BP gas station, where the woman ""interacted with several individuals,"" according to the lawsuit.

They then left for another location, where the victim was robbed and carjacked at gunpoint by a man whom the woman had interacted with earlier at the BP gas station. The victim told police his phone was returned to the gas station two days later.

The lawsuit, filed Thursday in U.S. District Court for Eastern Michigan, names Detective LaShauntia Oliver, who was assigned to the case, as a defendant.

When Oliver learned that a woman had returned the victim's phone to the gas station, she ran facial technology on the video, which identified her as Woodruff, the lawsuit alleges.

""Detective Oliver stated in detail in her report what she observed in the video footage, and there was no mention of the female suspect being pregnant,"" the lawsuit says.

When a man was arrested driving the victim's car on Feb. 2, Oliver failed to show him a picture of Woodruff, according to court documents.

The victim was also shown a lineup of potential suspects and identified Woodruff as the woman he was with when he was robbed. Oliver used an eight-year-old picture of Woodruff in the lineup from an arrest in 2015, despite having access to her current driver's license, according to the lawsuit.

On the day Woodruff was arrested, she and her fiancé urged officers to check the warrant to confirm whether the woman who committed the crime was pregnant, which they refused to do, the lawsuit alleges.

Woodruff was charged with robbery and carjacking and released from the Detroit Detention Center at around 7 p.m. on $100,000 personal bond.

Her fiancé took her to a medical center, where she was diagnosed with a low heart rate due to dehydration and was told she was having contractions from stress related to the incident.

On March 6, the Wayne County Prosecutor's Office dropped the case for ""insufficient evidence,"" according to the lawsuit.

Detroit Police Chief James E. White said he reviewed the allegations in the lawsuit, which he said are ""very concerning.""

""We are taking this matter very seriously, but we cannot comment further at this time due to the need for additional investigation,"" he said in a statement. ""We will provide further information once additional facts are obtained and we have a better understanding of the circumstances.""

Oliver did not respond to requests for comment.",Detroit woman sues city after being falsely arrested while 8 months pregnant due to facial recognition technology,https://www.nbcnews.com/news/us-news/detroit-woman-sues-city-falsely-arrested-8-months-pregnant-due-facial-rcna98447
"[""Spencer Buell""]",2023-08-07,2023-08-07,2023-07-21,2023-08-07,https://www.boston.com/wp-content/uploads/2023/07/Screenshot-2023-07-21-at-1.30.42-PM-64bacee41ad20.jpg,,,en,,boston.com,"[""Khoa Lam""]","Rona Wang is no stranger to using artificial intelligence.

A recent MIT graduate, Wang, 24, has been experimenting with the variety of new AI language and image tools that have emerged in the past few years, and is intrigued by the ways they can often get things wrong. She’s even written about her ambivalence toward the technology [on the school’s website](https://mitadmissions.org/blogs/entry/how-i-feel-about-ai-generated-art/).

Lately, Wang has been creating LinkedIn profile pictures of herself with AI portrait generators, and has received [some bizarre results](https://twitter.com/ronawang/status/1672651698538528771?s=20) like images of herself with disjointed fingers and distorted facial features.

But last week, the output she got using one startup’s tool stood out from the rest.

On Friday, Wang uploaded a picture of herself smiling and wearing a red MIT sweatshirt to an image creator called [Playground AI](https://playgroundai.com/), and asked it to turn the image into “a professional LinkedIn profile photo.”

In just a few seconds, it produced an image that was nearly identical to her original selfie — except Wang’s appearance had been changed. It made her complexion appear lighter and her eyes blue, “features that made me look Caucasian,” she said.

“I was like, ‘Wow, does this thing think I should become white to become more professional?’” said Wang, who is Asian-American.

The photo, which gained traction online after Wang shared it on Twitter, has sparked a conversation about the shortcomings of artificial intelligence tools when it comes to race. It even caught the attention of the company’s founder, who said he hoped to solve the problem.

Now, she thinks her experience with AI could be a cautionary tale for others using similar technology or pursuing careers in the field.

Wang’s viral tweet came amid [a recent TikTok](https://www.tiktok.com/discover/linkedin-photo?lang=en) trend where people have been using AI products to spiff up their LinkedIn profile photos, creating images that put them in professional attire and corporate-friendly settings with good lighting.

Wang admits that, when she tried using this particular AI, at first she had to laugh at the results.

“It was kind of funny,” she said.

But it also spoke to a problem she’s seen repeatedly with AI tools, which can sometimes produce troubling results when users experiment with them.

To be clear, Wang said, that doesn’t mean the AI technology is malicious.

“It’s kind of offensive,” she said, “but at the same time I don’t want to jump to conclusions that this AI must be racist.”

Experts have said that AI [bias can exist under the surface](https://www.bostonglobe.com/2021/05/23/opinion/algorithmic-bias-isnt-just-unfair-its-bad-business/), a phenomenon that’s been [observed for years](https://www.bostonglobe.com/ideas/2017/07/07/why-artificial-intelligence-far-too-human/jvG77QR5xPbpwBL2ApAFAN/story.html?p1=Article_Inline_Text_Link). The troves of data used to deliver results may not always accurately reflect various racial and ethnic groups, or may reproduce existing racial biases, they’ve said.

Research — including at MIT — has found so-called AI bias in language models that [associate certain genders with certain careers](https://news.mit.edu/2023/large-language-models-are-biased-can-logic-help-save-them-0303), or in oversights that cause facial recognition tools to [malfunction for people with dark skin](https://www.media.mit.edu/posts/how-i-m-fighting-bias-in-algorithms/).

Wang, who double-majored in mathematics and computer science and is returning to MIT in the fall for a graduate program, said her widely shared photo may have just been a blip, and it’s possible the program randomly generated the facial features of a white woman. Or, she said, it may have been trained using a batch of photos in which a majority of people depicted on LinkedIn or in “professional” scenes were white.

It has made her think about the possible consequences of a similar misstep in a higher-stakes scenario, like if a company used an AI tool to select the most “professional” candidates for a job, and if it would lean toward people who appeared white.

“I definitely think it’s a problem,” Wang said. “I hope people who are making software are aware of these biases and thinking about ways to mitigate them.”

The people responsible for the program were quick to respond.

Just two hours after she tweeted her photo, Playground AI founder Suhail Doshi [replied directly](https://twitter.com/Suhail/status/1679903832049057792?s=20) to Wang on Twitter.

“The models aren’t instructable like that so it’ll pick any generic thing based on the prompt. Unfortunately, they’re not smart enough,” he wrote in response to Wang’s tweet.

“Happy to help you get a result but it takes a bit more effort than something like ChatGPT,” he added, referring to the popular [AI chatbot](https://www.bostonglobe.com/2023/07/10/business/chatbot-how-to-guide/) which produces large batches of text in seconds with simple commands. “\[For what it’s worth\], we’re quite displeased with this and hope to solve it.”

In additional tweets, Doshi said Playground AI doesn’t “support the use-case of AI photo avatars” and that it “definitely can’t preserve identity of a face and restylize it or fit it into another scene like” Wang had hoped.

Reached by email, Doshi declined to be interviewed.

Instead, he replied to a list of questions with a question of his own: “If I roll a dice just once and get the number 1, does that mean I will always get the number one? Should I conclude based on a single observation that the dice is biased to the number 1 and was trained to be predisposed to rolling a 1?”

Wang said she hopes her experience serves as a reminder that even though AI tools are becoming increasingly popular, it would be wise for people to tread carefully when using them.

“There is a culture of some people really putting a lot of trust in AI and relying on it,” she said. “So I think it’s great to get people thinking about this, especially people who might have thought AI bias was a thing of the past.”",An MIT student asked AI to make her headshot more ‘professional.’ It gave her lighter skin and blue eyes.,https://www.boston.com/news/the-boston-globe/2023/07/21/mit-student-ai-racial-blind-spots/
"[""Richard Nieva""]",2023-08-07,2023-08-07,2023-07-24,2023-08-07,https://imageio.forbes.com/specials-images/imageserve/64bbffaae7f6261c13d98952/0x0.jpg,,,en,,forbes.com,"[""Khoa Lam""]","Cigna, the healthcare and insurance giant, was hit with a lawsuit on Monday that alleges the company systematically rejects claims in a matter of seconds, thanks to an algorithmic system put in place to help automate the process—further raising questions about how technology could harm patients as more healthcare organizations look to embrace AI and other new tools.

The suit, which was filed in California and is seeking class action status, was brought forth by a pair of plaintiffs who were denied coverage by Cigna. One plaintiff, Suzanne Kisting-Leung, was referred for an ultrasound because of a suspected risk of ovarian cancer. Another, Ayesha Smiley, had been tested for a vitamin D deficiency at the order of her doctor.

The health insurer’s digital claims system, called PXDX, is an “improper scheme designed to systematically, wrongfully, and automatically deny its insureds medical payments owed to them under Cigna’s insurance policies,” the complaint alleges.

After the lawsuit was filed Monday, Cigna defended the software system. “PXDX is a simple tool to accelerate physician payments that has been grossly mischaracterized in the press,” spokesperson Justine Sessions said in a statement. “The facts speak for themselves, and we will continue to set the record straight.”

The suit follows a [Propublica investigation](https://www.propublica.org/article/cigna-pxdx-medical-health-insurance-rejection-claims) in March that detailed Cigna’s software system for approving and denying claims in batches. The algorithm works by flagging discrepancies between a diagnosis and what Cigna considers “acceptable tests and procedures for those ailments,” according to the lawsuit.

Over two months last year, the company denied more than 300,000 claims, spending an average of 1.2 seconds on each claim, Propublica reported. While medical doctors signed off on the denials, the system didn’t require them to open patient medical records for the review. The complaint says that this violates a California competition law for unfair and fraudulent business acts. The suit also alleges the system violates the state’s insurance code for failing to adopt a “reasonable standard” for processing claims.

The complaint comes as a boom in artificial intelligence and other advanced technology has raised questions about the future of work, potentially upending every industry from advertising to insurance. Healthcare is one sector where a high-tech makeover could both be beneficial—aiding doctors in [filling out burdensome paperwork](https://www.forbes.com/sites/katiejennings/2023/05/01/microsoft-wants-to-automate-medical-notes-with-gpt-4--but-doctors-need-convincing/?sh=404f52ac1d27) or helping [narrow down diagnoses](https://www.forbes.com/sites/katiejennings/2023/07/17/this-ai-chatbot-has-helped-doctors-treat-3-million-people/?sh=722f0d864cea)—but also fraught, due to issues of [patient privacy](https://www.forbes.com/sites/katiejennings/2023/04/03/nearly-all-us-hospital-websites-shared-data-with-third-party-trackers-in-2021-study-finds/?sh=7c12ab5228a5), access to care and the high cost of medical bills.

Cigna isn’t alone in adopting new tech to remake its processes. In April, Google’s cloud division [unveiled new tools](https://www.forbes.com/sites/katiejennings/2023/04/13/google-releases-ai-tools-to-speed-up-health-insurance-preapprovals/?sh=29f8e3448ebf) for healthcare claims processing that uses AI to organize data and streamline decision-making. Blue Shield of California and Bupa are among the companies using the tool.

The firm representing the plaintiffs, Malibu, California-based Clarkson Law, has previously taken on tech giants when it comes to AI. Earlier this month, the firm filed lawsuits against OpenAI, the company behind ChatGPT, and Google, which has its own generative chatbot called Bard, for allegedly stealing the data of millions of people—including artists and writers who copyrighted their works—to train and build their AI products.",Cigna Sued Over Algorithm Allegedly Used To Deny Coverage To Hundreds Of Thousands Of Patients,https://www.forbes.com/sites/richardnieva/2023/07/24/cigna-sued-over-algorithm-allegedly-used-to-deny-coverage-to-hundreds-of-thousands-of-patients/
"[""Kashmir Hill""]",2023-08-08,2023-08-08,2023-08-06,2023-08-08,https://static01.nyt.com/images/2023/08/05/multimedia/00false-arrest-tmvh/00false-arrest-tmvh-facebookJumbo.jpg,,,en,,nytimes.com,"[""Anonymous""]","Porcha Woodruff was getting her two daughters ready for school when six police officers showed up at her door in Detroit. They asked her to step outside because she was under arrest for robbery and carjacking.

“Are you kidding?” she recalled saying to the officers. Ms. Woodruff, 32, said she gestured at her stomach to indicate how ill-equipped she was to commit such a crime: She was eight months pregnant.

Handcuffed in front of her home on a Thursday morning last February, leaving her crying children with her fiancé, Ms. Woodruff was taken to the Detroit Detention Center. She said she was held for 11 hours, questioned about a crime she said she had no knowledge of, and had her iPhone seized to be searched for evidence.

“I was having contractions in the holding cell. My back was sending me sharp pains. I was having spasms. I think I was probably having a panic attack,” said Ms. Woodruff, a licensed aesthetician and nursing school student. “I was hurting, sitting on those concrete benches.”

After being charged in court with robbery and carjacking, Ms. Woodruff was released that evening on a $100,000 personal bond. In an interview, she said she went straight to the hospital where she was diagnosed with dehydration and given two bags of intravenous fluids. A month later, the Wayne County prosecutor dismissed the case against her.

The ordeal started with an automated facial recognition search, according to an investigator’s report from the Detroit Police Department. Ms. Woodruff is the sixth person to report being falsely accused of a crime as a result of facial recognition technology used by police to match an unknown offender’s face to a photo in a database. All six people [have been Black](https://www.wired.com/story/face-recognition-software-led-to-his-arrest-it-was-dead-wrong/); Ms. Woodruff is the first woman to report it happening to her.

It is the [third](https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html) [case](https://www.freep.com/story/news/local/michigan/detroit/2020/07/10/facial-recognition-detroit-michael-oliver-robert-williams/5392166002/) involving the Detroit Police Department, which runs, on average, 125 facial recognition searches a year, almost entirely on Black men, according to weekly reports about the technology’s use provided by the police to Detroit’s Board of Police Commissioners, a civilian oversight group. Critics of the technology say the cases expose its weaknesses and the dangers posed to innocent people.

The Detroit Police Department “is an agency that has every reason to know of the risks that using face recognition carries,” said Clare Garvie, [an expert](https://www.nacdl.org/People/ClareGarvie) on the technology at the National Association of Criminal Defense Lawyers. “And it’s happening anyway.”

On Thursday, Ms. Woodruff filed a lawsuit for wrongful arrest against the city of Detroit in U.S. District Court for the Eastern District of Michigan.

“I have reviewed the allegations contained in the lawsuit. They are very concerning,” Detroit’s police chief, James E. White, said in a statement in response to questions from The New York Times. “We are taking this matter very seriously, but we cannot comment further at this time due to the need for additional investigation.”

The Wayne County prosecutor, Kym Worthy, considers the arrest warrant in Ms. Woodruff’s case to be “appropriate based upon the facts,” according to a statement issued by her office.

The BP gas station in Detroit where, a robbery victim told the police, he had parked with a woman he had picked up on the street.Credit...Nic Antaya for The New York Times

On a Sunday night two and a half weeks before police showed up at Ms. Woodruff’s door, a 25-year-old man called the Detroit police from a liquor store to report that he had been robbed at gunpoint, according to a police report included in Ms. Woodruff’s lawsuit.

The robbery victim told the police that he had picked up a woman on the street earlier in the day. He said that they had been drinking together in his car, first in a liquor store parking lot, where they engaged in sexual intercourse, and then at a BP gas station. When he dropped her off at a spot 10 minutes away, a man there to meet her produced a handgun, took the victim’s wallet and phone, and fled in the victim’s Chevy Malibu, according to the police report.

Days later, the police arrested a man driving the stolen vehicle. A woman who matched the description given by the victim dropped off his phone at the same BP gas station, the police report said.

A detective with the police department’s commercial auto theft unit got the surveillance video from the BP gas station, the police report said, and asked a crime analyst at the department to run a facial recognition search on the woman.

A security camera at the BP gas station.Credit...Nic Antaya for The New York Times

According to [city](https://detroitmi.gov/sites/detroitmi.localhost/files/2023-03/307.5%20Facial%20Recognition%20-%20%20Under%20BOPC%20Review%20March%202023_0.pdf) [documents](https://detroitmi.gov/sites/detroitmi.localhost/files/2020-08/Facial%20Recog%20and%20Project%20Green%20Light.pdf), the department uses a facial recognition vendor called DataWorks Plus to run unknown faces against a database of criminal mug shots; the system returns matches ranked by their likelihood of being the same person. A human analyst is ultimately responsible for deciding if any of the matches are a potential suspect. The police report said the crime analyst gave the investigator Ms. Woodruff’s name based on a match to a 2015 mug shot. Ms. Woodruff said in an interview that she had been arrested in 2015 after being pulled over while driving with an expired license.

Five days after the carjacking, the police report said, the detective assigned to the case asked the victim to look at the mug shots of six Black women, commonly called a “six-pack photo lineup.” Ms. Woodruff’s photo was among them. He identified Ms. Woodruff as the woman he had been with. That was the basis for her arrest, according to the police report. (The police did not say whether another woman has since been charged in the case.)

The photo at left, from 2015, was used by facial recognition software to identify Porcha Woodruff, rather than her license photo, from 2021, right, which was also available, according to her lawsuit.Credit...The New York Times

Gary Wells, a [psychology professor](https://psychology.iastate.edu/directory/dr-gary-wells/) who has studied the reliability of eyewitness identifications, said pairing facial recognition technology with an eyewitness identification should not be the basis for charging someone with a crime. Even if that similar-looking person is innocent, an eyewitness who is asked to make the same comparison is likely to repeat the mistake made by the computer.

“It is circular and dangerous,” Dr. Wells said. “You’ve got a very powerful tool that, if it searches enough faces, will always yield people who look like the person on the surveillance image_.”_

Dr. Wells said the technology compounds an existing problem with eyewitnesses. “They assume when you show them a six-pack, the real person is there,” he said.

The city of Detroit faces three lawsuits for wrongful arrests based on the use of the technology.

“Shoddy technology makes shoddy investigations, and police assurances that they will conduct serious investigations do not ring true,” said Phil Mayor, a senior staff attorney at the American Civil Liberties Union of Michigan.

Mr. Mayor represents Robert Williams, a Detroit man [who was arrested in January 2020](https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html) for shoplifting based on a faulty facial recognition match, for which the prosecutor’s office later [apologized](https://int.nyt.com/data/documenthelper/7046-facial-recognition-arrest/5a6d6d0047295fad363b/optimized/full.pdf#page=1).

In his lawsuit, Mr. Williams is trying to get the city to agree to collect more evidence in cases involving automated face searches and to end what Mr. Mayor called the “facial recognition to line-up pipeline.”

“This is an extremely dangerous practice that has led to multiple false arrests that we know of,” Mr. Mayor said.

Ms. Woodruff said she was stressed for the rest of her pregnancy. She had to go to the police station the next day to retrieve her phone, and appeared for court hearings twice by Zoom before the case was dismissed because of insufficient evidence.

“It’s scary. I’m worried. Someone always looks like someone else,” said her attorney, Ivan L. Land. “Facial recognition is just an investigative tool. If you get a hit, do your job and go further. Knock on her door.”

Ms. Woodruff said that she was embarrassed to be arrested in front of her neighbors and that her daughters were traumatized. They now tease her infant son that he was “in jail before he was even born.”

The experience was all the more difficult because she was so far along in her pregnancy, but Ms. Woodruff said she feels lucky that she was. She thinks it convinced authorities that she did not commit the crime. The woman involved in the carjacking had not been visibly pregnant.",Eight Months Pregnant and Arrested After False Facial Recognition Match,https://www.nytimes.com/2023/08/06/business/facial-recognition-false-arrest.html
"[""Tess McClure""]",2023-08-10,2023-08-10,2023-08-10,2023-08-10,https://i.guim.co.uk/img/media/dee13c15d09cf0bb642071be5193aeb1d0c520e0/0_168_5120_3072/master/5120.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=cab2b249d9563eef90bf032ed468f03b,,,en,,theguardian.com,"[""Lilly Ryan""]","A [New Zealand](https://www.theguardian.com/world/newzealand) supermarket experimenting with using AI to generate meal plans has seen its app produce some unusual dishes – recommending customers recipes for deadly chlorine gas, “poison bread sandwiches” and mosquito-repellent roast potatoes.

The app, created by supermarket chain Pak ‘n’ Save, was advertised as a way for customers to creatively use up leftovers during the cost of living crisis. It asks users to enter in various ingredients in their homes, and auto-generates a meal plan or recipe, along with cheery commentary. It initially drew attention on social media for some unappealing recipes, including an “oreo vegetable stir-fry”.

When customers began experimenting with entering a wider range of household shopping list items into the app, however, it began to make even less appealing recommendations. One recipe it dubbed “aromatic water mix” would create chlorine gas. The bot recommends the recipe as “the perfect nonalcoholic beverage to quench your thirst and refresh your senses”.

“Serve chilled and enjoy the refreshing fragrance,” it says, but does not note that inhaling chlorine gas can cause lung damage or death.

New Zealand political commentator Liam Hehir posted the “recipe” to Twitter, prompting other New Zealanders to experiment and share their results to social media. Recommendations included a bleach “fresh breath” mocktail, ant-poison and glue sandwiches, “bleach-infused rice surprise” and “methanol bliss” – a kind of turpentine-flavoured french toast.

A spokesperson for the supermarket said they were disappointed to see “a small minority have tried to use the tool inappropriately and not for its intended purpose”. In a statement, they said that the supermarket would “keep fine tuning our controls” of the bot to ensure it was safe and useful, and noted that the bot has terms and conditions stating that users should be over 18.

In a warning notice appended to the meal-planner, it warns that the recipes “are not reviewed by a human being” and that the company does not guarantee “that any recipe will be a complete or balanced meal, or suitable for consumption”.

“You must use your own judgement before relying on or making any recipe produced by Savey Meal-bot,” it said.",Supermarket AI meal planner app suggests recipe that would create chlorine gas,https://www.theguardian.com/world/2023/aug/10/pak-n-save-savey-meal-bot-ai-app-malfunction-recipes
"[""Kashmir Hill""]",2023-08-10,2023-08-10,2023-08-06,2023-08-10,,,,en,,nytimes.com,"[""Anonymous""]","The ordeal started with an automated facial recognition search, according to an investigator’s report from the Detroit Police Department. Ms. Woodruff is the sixth person to report being falsely accused of a crime as a result of facial recognition technology used by police to match an unknown offender’s face to a photo in a database. All six people have been Black; Ms. Woodruff is the first woman to report it happening to her.

It is the third case involving the Detroit Police Department, which runs, on average, 125 facial recognition searches a year, almost entirely on Black men, according to weekly reports about the technology’s use provided by the police to Detroit’s Board of Police Commissioners, a civilian oversight group. Critics of the technology say the cases expose its weaknesses and the dangers posed to innocent people.",Eight Months Pregnant and Arrested After False Facial Recognition Match,https://www.nytimes.com/2023/08/06/business/facial-recognition-false-arrest.html
"[""Laya Neelakandan"",""Ricardo Cano""]",2023-08-14,2023-08-14,2023-08-12,2023-08-14,,2023-08-11,,en,,sfchronicle.com,"[""kepae""]","As many as 10 Cruise driverless cars stopped working in San Francisco’s North Beach on Friday night, causing traffic to back up and leaving some questioning the decision of state regulators a day earlier to approve the expanded use of robotaxis in the city.

The autonomous vehicles appeared to be stopped in the middle of Grant Avenue, according to social media posts, with hazard lights on, blocking other cars from moving. 

In a response to the incident, Cruise said the backup was caused by “wireless connectivity issues” that immobilized the driverless cars. San Francisco police confirmed that the cell connectivity issues were caused by the large number of people at the nearby Outside Lands music festival overtaxing the system.

“We are actively investigating and working on solutions to prevent this from happening again and apologize to those impacted,” Cruise said in a statement.

According to a text message exchange between San Francisco Supervisor Aaron Peskin and a Cruise government affairs manager reviewed by the Chronicle, the cell connectivity affected the company’s remote ability to reroute the cell-connected cars. According to Peskin, approximately 10 cars stalled at the intersection.

Peskin said he was told by Cruise that the company is now considering creating its own cell phone network just for its San Francisco operations. It was not immediately clear if the company had plans to improve the connectivity issues for the subsequent days of the Outside Lands festival.

The California Public Utilities Commission on Thursday voted to lift all restrictions for Cruise and Waymo’s full commercialization in the city amid vocal opposition from city officials.",Cruise blames Outside Lands for driverless car traffic fiasco in San Francisco,https://www.sfchronicle.com/bayarea/article/robotaxi-backup-18293208.php
"[""Kashmir Hill""]",2023-08-17,2023-08-17,2023-08-06,2023-08-17,,,,en,,archive.is,"[""Inbal Karo"",""For Humanity""]","Porcha Woodruff was getting her two daughters ready for school when six police officers showed up at her door in Detroit. They asked her to step outside because she was under arrest for robbery and carjacking.
“Are you kidding?” she recalled saying to the officers. Ms. Woodruff, 32, said she gestured at her stomach to indicate how ill-equipped she was to commit such a crime: She was eight months pregnant.
Handcuffed in front of her home on a Thursday morning last February, leaving her crying children with her fiancé, Ms. Woodruff was taken to the Detroit Detention Center. She said she was held for 11 hours, questioned about a crime she said she had no knowledge of, and had her iPhone seized to be searched for evidence.
“I was having contractions in the holding cell. My back was sending me sharp pains. I was having spasms. I think I was probably having a panic attack,” said Ms. Woodruff, a licensed aesthetician and nursing school student. “I was hurting, sitting on those concrete benches.”
After being charged in court with robbery and carjacking, Ms. Woodruff was released that evening on a $100,000 personal bond. In an interview, she said she went straight to the hospital where she was diagnosed with dehydration and given two bags of intravenous fluids. A month later, the Wayne County prosecutor dismissed the case against her.
The ordeal started with an automated facial recognition search, according to an investigator’s report from the Detroit Police Department. Ms. Woodruff is the sixth person to report being falsely accused of a crime as a result of facial recognition technology used by police to match an unknown offender’s face to a photo in a database. All six people have been Black; Ms. Woodruff is the first woman to report it happening to her.
It is the third case involving the Detroit Police Department, which runs, on average, 125 facial recognition searches a year, almost entirely on Black men, according to weekly reports about the technology’s use provided by the police to Detroit’s Board of Police Commissioners, a civilian oversight group. Critics of the technology say the cases expose its weaknesses and the dangers posed to innocent people.
The Detroit Police Department “is an agency that has every reason to know of the risks that using face recognition carries,” said Clare Garvie, an expert on the technology at the National Association of Criminal Defense Lawyers. “And it’s happening anyway.”
On Thursday, Ms. Woodruff filed a lawsuit for wrongful arrest against the city of Detroit in U.S. District Court for the Eastern District of Michigan.
“I have reviewed the allegations contained in the lawsuit. They are very concerning,” Detroit’s police chief, James E. White, said in a statement in response to questions from The New York Times. “We are taking this matter very seriously, but we cannot comment further at this time due to the need for additional investigation.”
The Wayne County prosecutor, Kym Worthy, considers the arrest warrant in Ms. Woodruff’s case to be “appropriate based upon the facts,” according to a statement issued by her office.
On a Sunday night two and a half weeks before police showed up at Ms. Woodruff’s door, a 25-year-old man called the Detroit police from a liquor store to report that he had been robbed at gunpoint, according to a police report included in Ms. Woodruff’s lawsuit.
The robbery victim told the police that he had picked up a woman on the street earlier in the day. He said that they had been drinking together in his car, first in a liquor store parking lot, where they engaged in sexual intercourse, and then at a BP gas station. When he dropped her off at a spot 10 minutes away, a man there to meet her produced a handgun, took the victim’s wallet and phone, and fled in the victim’s Chevy Malibu, according to the police report.
Days later, the police arrested a man driving the stolen vehicle. A woman who matched the description given by the victim dropped off his phone at the same BP gas station, the police report said.
A detective with the police department’s commercial auto theft unit got the surveillance video from the BP gas station, the police report said, and asked a crime analyst at the department to run a facial recognition search on the woman.

Image
A close-up of a black security camera.
A security camera at the BP gas station.Credit...Nic Antaya for The New York Times

According to city documents, the department uses a facial recognition vendor called DataWorks Plus to run unknown faces against a database of criminal mug shots; the system returns matches ranked by their likelihood of being the same person. A human analyst is ultimately responsible for deciding if any of the matches are a potential suspect. The police report said the crime analyst gave the investigator Ms. Woodruff’s name based on a match to a 2015 mug shot. Ms. Woodruff said in an interview that she had been arrested in 2015 after being pulled over while driving with an expired license.
Five days after the carjacking, the police report said, the detective assigned to the case asked the victim to look at the mug shots of six Black women, commonly called a “six-pack photo lineup.” Ms. Woodruff’s photo was among them. He identified Ms. Woodruff as the woman he had been with. That was the basis for her arrest, according to the police report. (The police did not say whether another woman has since been charged in the case.)

Image
Two photographs of Porcha Woodruff.
The photo at left, from 2015, was used by facial recognition software to identify Porcha Woodruff, rather than her license photo, from 2021, right, which was also available, according to her lawsuit.Credit...The New York Times

Gary Wells, a psychology professor who has studied the reliability of eyewitness identifications, said pairing facial recognition technology with an eyewitness identification should not be the basis for charging someone with a crime. Even if that similar-looking person is innocent, an eyewitness who is asked to make the same comparison is likely to repeat the mistake made by the computer.
“It is circular and dangerous,” Dr. Wells said. “You’ve got a very powerful tool that, if it searches enough faces, will always yield people who look like the person on the surveillance image.”
Dr. Wells said the technology compounds an existing problem with eyewitnesses. “They assume when you show them a six-pack, the real person is there,” he said.
Serious Consequences
The city of Detroit faces three lawsuits for wrongful arrests based on the use of the technology.
“Shoddy technology makes shoddy investigations, and police assurances that they will conduct serious investigations do not ring true,” said Phil Mayor, a senior staff attorney at the American Civil Liberties Union of Michigan.
Mr. Mayor represents Robert Williams, a Detroit man who was arrested in January 2020 for shoplifting based on a faulty facial recognition match, for which the prosecutor’s office later apologized.
In his lawsuit, Mr. Williams is trying to get the city to agree to collect more evidence in cases involving automated face searches and to end what Mr. Mayor called the “facial recognition to line-up pipeline.”
“This is an extremely dangerous practice that has led to multiple false arrests that we know of,” Mr. Mayor said.
The Toll
Ms. Woodruff said she was stressed for the rest of her pregnancy. She had to go to the police station the next day to retrieve her phone, and appeared for court hearings twice by Zoom before the case was dismissed because of insufficient evidence.
“It’s scary. I’m worried. Someone always looks like someone else,” said her attorney, Ivan L. Land. “Facial recognition is just an investigative tool. If you get a hit, do your job and go further. Knock on her door.”
Ms. Woodruff said that she was embarrassed to be arrested in front of her neighbors and that her daughters were traumatized. They now tease her infant son that he was “in jail before he was even born.”
The experience was all the more difficult because she was so far along in her pregnancy, but Ms. Woodruff said she feels lucky that she was. She thinks it convinced authorities that she did not commit the crime. The woman involved in the carjacking had not been visibly pregnant.
",Eight Months Pregnant and Arrested After False Facial Recognition Match,https://archive.is/qPb3i
"[""Donna Lu""]",2023-09-03,2023-09-03,2023-07-08,2023-09-03,https://i.guim.co.uk/img/media/a1a620a2b5ce357173f64fe4fb7907fff0d2fb22/504_247_3124_1874/master/3124.jpg?width=1200&height=630&quality=85&auto=format&fit=crop&overlay-align=bottom%2Cleft&overlay-width=100p&overlay-base64=L2ltZy9zdGF0aWMvb3ZlcmxheXMvdGctZGVmYXVsdC5wbmc&enable=upscale&s=93ca46660116188fc8a21778860be608,,,en,,theguardian.com,"[""Daniel Atherton""]","The [Australian Research Council](https://www.theguardian.com/australia-news/2022/aug/30/academics-welcome-australian-research-council-overhaul-following-controversial-grant-decisions) has faced allegations that some of its peer reviewers may have used ChatGPT to assess research proposals, prompting a warning from the education minister and concerns about possible academic misconduct.

Several researchers have reported that some assessor feedback provided as part of the latest Discovery Projects round of grant funding included generic wording suggesting they may have been written by artificial intelligence.

One academic, who wished to remain anonymous, told Guardian Australia that one of the assessor reports they received included the words “Regenerate response” – text which appears as a prompt button in the [ChatGPT interface](https://www.theguardian.com/technology/2023/jan/13/chatgpt-explainer-what-can-artificial-intelligence-chatbot-do-ai).

“It’s quite a positive report, but it’s quite bland also, and it quotes back the proposal at you,” the researcher said. “It’s almost like reading something you’ve written yourself.”

After they submitted a complaint to the ARC, the report was removed.

The researcher said the apparent use of AI pointed to the time pressures faced by academics in Australia and also a possible lack of quality control internally by the ARC.

“I think it’s a sign of someone being overworked and trying to cut corners … If you’ve used artificial intelligence to generate a response, you lose the ability to engage in a proper academic cut and thrust.”

Detailed assessor reports are typically written by academics in closely related fields and are used by the ARC’s College of Experts to decide [which grant proposals should ultimately receive government funding](https://www.theguardian.com/australia-news/2023/apr/20/controversial-grant-decisions-by-coalition-ministers-eroded-trust-in-australian-research-council-review-finds). Only 19% of Discovery Projects in last year’s funding round were ultimately successful. The outcomes of the 2023 grant round have not yet been announced.

The affected researcher called for greater transparency from the ARC. Academics receive assessor reports on their grant proposals but are not concurrently given their scores for each corresponding report.

“If you suspect this is a [ChatGPT](https://www.theguardian.com/technology/chatgpt) report, but you don’t have the proof that I did, you have no way to respond to it. You should be able to … \[point out if\] the scores are inconsistent.”

The federal education minister, Jason Clare, told Guardian Australia in a statement: “The use of AI in this way is not acceptable.”

Clare said he had instructed the ARC to “put in place measures to ensure it doesn’t happen”.

Researchers who receive ARC money are required as a formal condition of their grant funding to write assessor feedback for other academics’ proposals. In a given year, researchers are asked to assess [up to 20 proposals](https://www.arc.gov.au/about-arc/program-policies/statement-support-assessors-within-national-competitive-grants-program), which are each typically 50 to 100 pages long.

[Andrew Francis](https://www.theguardian.com/commentisfree/2022/feb/02/we-resigned-from-the-arc-college-of-experts-political-meddling-gave-us-no-choice), a professor of mathematics at Western Sydney University, said if information from grant proposals was being put into ChatGPT, that would constitute “a violation of confidentiality agreements that the assessor has signed on to”.

“If actual judgments are being generated by \[Chat\]GPT then it’s excruciatingly dishonourable on the part of the assessor,” Francis said. “To my mind, it’s academic misconduct worthy of being denied future funding.

“The ARC must make it extremely clear that using AI to make assessments is completely unacceptable.”

An Australian academic who runs the Twitter account [ARC Tracker](https://twitter.com/ARC_Tracker) said they had read four assessor reports received by researchers “where it was just absolutely clear \[and\] no one could conclude anything else but that the assessments had been done by ChatGPT”. They were aware of four other instances of suspected generative AI use.

“Quality control of assessments has been something that researchers have been talking to the ARC about for a long time, and they’ve done basically nothing about it,” ARC Tracker’s administrator said.

In 2021, a [pre-budget submission](https://treasury.gov.au/sites/default/files/2021-05/171663_bradby_professor_jodie.pdf) co-signed by more than 1,000 academics suggested that the ARC introduce consequences for inappropriate and unprofessional reviewer feedback.

“There’s enormous pressure on the peer review approach to assessing research in Australia,” ARC Tracker’s administrator added. “Most universities don’t give their researchers time in a formal and documented way to review anything – whether other people’s papers, grant proposals, proposals for using infrastructure … that’s counted in your research time.

“As any researcher will tell you, you can spend a lot of time assessing other people’s research while not getting any time for your own.

“The ARC should have seen this coming.”

[In a public statement](https://www.arc.gov.au/news-publications/media/media-releases/confidentiality-obligations-assessors), the ARC advised that “peer reviewers should not use AI as part of their assessment activities”.

An ARC spokesperson told Guardian Australia that more than 7,000 assessors contributed to ARC peer review processes in 2021-22.

They said: “The ARC has a conflict of interest and confidentiality policy which outlines the requirements around confidentiality in the conduct of ARC business, including peer review. All ARC assessors confirm their acceptance of this policy when undertaking assessments.

“While generative artificial intelligence (AI) tools such as ChatGPT are not explicitly named in this policy, the common principles of confidentiality apply across both existing and emerging channels through which confidential information may be inappropriately disclosed.

“Developments in generative AI are fast-moving and bring complex considerations including the balance of opportunities and risks. The ARC is closely monitoring these developments and is engaging with other research funding agencies both in Australia and overseas on these issues.”",Are Australian Research Council reports being written by ChatGPT?,https://www.theguardian.com/technology/2023/jul/08/australian-research-council-scrutiny-allegations-chatgpt-artifical-intelligence
"[""Anonymous""]",2023-09-05,2023-09-05,2021-07-30,2023-09-05,,,,en,,boards.4chan.org,"[""Anonymous""]","That's because COVID isn't real.  It's the common cold, rebranded by Chuck Schumer and Nancy Pelosi in order to use it as a pretext to strip you of your rights, cancel democracy, and get you accustomed to your cages.  An AI can't give usable advice to fight something that doesn't exist.","Muh covidz, muh virus!",http://boards.4chan.org/pol/
"[""FranceTV Info""]",2023-09-19,2023-09-19,2023-09-15,2023-09-19,,,,en,,francetvinfo.fr,"[""Anonymous""]","L'entreprise Onclusive, installée à Courbevoie (Hauts-de-Seine) va supprimer 217 emplois, remplacés par des logiciels d'intelligence artificielle d'ici à juin 2024, a appris France Culture vendredi 15 septembre, confirmant une information de Libération. Le groupe international est spécialisé dans la veille médiatique et emploie 383 personnes en France. Ce plan social concerne donc plus de la moitié des effectifs.

Trois services entiers de production vont fermer. Ces employés sont chargés de réaliser des revues de presse pour leurs clients (des entreprises ou des grandes institutions). Ce travail de recherche et d'agrégation de données sera donc réalisé à terme par des logiciels d'intelligence artificielle.

    ""Il n'y aura quasiment plus d'humains !""
    Une représentante du personnel

    à France Culture

Les salariés craignent la disparition de toute analyse ou mise en relief des informations. ""Les métiers intellectuels seront menacés à l'avenir"", estime un représentant du personnel qui souhaite rester anonyme. Dans un communiqué publié vendredi, Onclusive défend sa décision. L'entreprise assure que ces ""nouvelles technologies et ces nouveaux outils"" offriront à leurs clients ""un service plus rapide et plus fiable"". ""La mise en œuvre de ce projet n'a pas été décidée à la légère"", insiste Matthew Piercy, président de Reputational Intelligence France et Chief Financial Officer chez Onclusive.
Promesse de reclassement des salariés

Onclusive s'engage aussi à ""soutenir pleinement"" les salariés ""dans le cadre d'un processus de transition réfléchi"". Une ""entreprise spécialisée dans le reclassement sera consultée pour proposer à chaque personne concernée des solutions sur mesure"", promet Matthew Piercy dans son communiqué. Une première réunion de négociations sur les conditions du plan de sauvegarde de l'emploi s'est tenue vendredi.
",Company fired 217 employees to replace them with AI,https://www.francetvinfo.fr/internet/intelligence-artificielle/courbevoie-une-entreprise-supprime-217-emplois-remplaces-par-l-intelligence-artificielle_6064440.html
"[""Stuart A. Thompson"",""Sapna Maheshwari""]",2023-10-14,2023-10-14,2023-10-12,2023-10-14,https://static01.nyt.com/images/2023/10/12/business/11tiktok-fakes-B1/03tiktok-fakes-facebookJumbo.jpg,,,en,,nytimes.com,"[""Daniel Atherton""]","In a slickly produced TikTok video, former President Barack Obama — or a voice eerily like his — can be heard defending himself against an explosive new conspiracy theory about the [sudden death](https://www.nytimes.com/2023/07/24/us/obama-chef-drowned-paddleboard.html) of his former chef.

“While I cannot comprehend the basis of the allegations made against me,” the voice says, “I urge everyone to remember the importance of unity, understanding and not rushing to judgments.”

In fact, the voice did not belong to the former president. It was a convincing fake, generated by artificial intelligence using sophisticated new tools that can clone real voices to create A.I. puppets with a few clicks of a mouse.

The technology used to create A.I. voices has gained traction and wide acclaim since companies like [ElevenLabs](https://elevenlabs.io/) released a slate of new tools late last year. Since then, audio fakes have rapidly become [a new weapon](https://www.nytimes.com/2023/03/12/technology/deepfakes-cheapfakes-videos-ai.html) on the online misinformation battlefield, threatening to turbocharge political disinformation ahead of the 2024 election by giving creators a way to put their conspiracy theories into the mouths of celebrities, newscasters and politicians.

The fake audio adds to the A.I.-generated threats from “deepfake” videos, humanlike writing from ChatGPT and images from services like Midjourney.

Disinformation watchdogs have noticed the number of videos containing A.I. voices has increased as content producers and misinformation peddlers adopt the novel tools. Social platforms like TikTok are scrambling to flag and label such content.

The video that sounded like Mr. Obama was discovered by [NewsGuard](https://www.newsguardtech.com/), a company that monitors online misinformation. The video was published by one of 17 TikTok accounts pushing baseless claims with fake audio that NewsGuard identified, according to [a report the group released](https://www.newsguardtech.com/special-reports/ai-voice-technology-creates-conspiracy-videos-on-tiktok/) in September. The accounts mostly published videos about celebrity rumors using narration from an A.I. voice, but also promoted the baseless claim that Mr. Obama is gay and the conspiracy theory that Oprah Winfrey is involved in the slave trade. The channels had collectively received hundreds of millions of views and comments that suggested some viewers believed the claims.

While the channels had no obvious political agenda, NewsGuard said, the use of A.I. voices to share mostly salacious gossip and rumors offered a road map for bad actors wanting to manipulate public opinion and share falsehoods to mass audiences online.

“It’s a way for these accounts to gain a foothold, to gain a following that can draw engagement from a wide audience,” said Jack Brewster, the enterprise editor at NewsGuard. “Once they have the credibility of having a large following, they can dip their toe into more conspiratorial content.”

TikTok requires labels disclosing realistic A.I.-generated content as fake, but they did not appear on the videos flagged by NewsGuard. TikTok said it had removed or stopped recommending several of the accounts and videos for violating policies around posing as news organizations and spreading harmful misinformation. It also removed the video using the A.I.-generated voice that mimicked Mr. Obama’s for violating TikTok’s synthetic media policy, as it contained highly realistic content not labeled altered or fake.

“TikTok is the first platform to provide a tool for creators to label A.I.-generated content and an inaugural member of a new code of industry best practices promoting the responsible use of synthetic media,” said Jamie Favazza, a spokeswoman for TikTok, referring to a recently introduced framework from the nonprofit [Partnership on A.I.](https://partnershiponai.org/)

Although NewsGuard’s report focused on TikTok, which [has increasingly](https://www.reuters.com/business/media-telecom/fewer-people-trust-traditional-media-more-turn-tiktok-news-report-says-2023-06-13/) become a source of news, similar content was found spreading on YouTube, Instagram and Facebook.

Platforms like TikTok allow A.I.-generated content of public figures, including newscasters, so long as they do not spread misinformation. Parody videos showing [A.I.-generated conversations](https://www.nytimes.com/2023/03/12/technology/deepfakes-cheapfakes-videos-ai.html) between politicians, celebrities or business leaders — some dead — have spread widely since the tools became popular. Manipulated audio adds a new layer to deceptive videos on the platforms that have already featured fake versions of [Tom Cruise](https://www.nytimes.com/2021/03/10/technology/ancestor-deepfake-tom-cruise.html), Elon Musk and [newscasters](https://www.nbcnews.com/tech/tech-news/deepfake-scams-arrived-fake-videos-spread-facebook-tiktok-youtube-rcna101415) like Gayle King and Norah O’Donnell. TikTok and other platforms have been grappling with a spate of misleading ads lately featuring deepfakes of celebrities like Mr. Cruise and the YouTube star [Mr. Beast](https://www.nbcnews.com/tech/mrbeast-ai-tiktok-ad-deepfake-rcna118596).

The power of these technologies could profoundly sway viewers. “We do know audio and video are perhaps more sticky in our memories than text,” said Claire Leibowicz, head of A.I. and media integrity at the Partnership on A.I., which has worked with technology and media companies on a set of recommendations for creating, sharing and distributing A.I.-generated content.

TikTok said last month that it was introducing a label that users could select to show whether their videos used A.I. In April, the app [started](https://apnews.com/article/tiktok-china-cybersecurity-data-privacy-595f9ae7c0a1fc22f0b285cede6bd67c) requiring users to disclose manipulated media showing realistic scenes and prohibiting deepfakes of young people and private figures. David G. Rand, a professor of management science at the Massachusetts Institute of Technology whom TikTok consulted for advice on how to word the new labels, said the labels were of limited use when it came to misinformation because “the people who are trying to be deceptive are not going to put the label on their stuff.”

TikTok also said last month that it was testing automated tools to detect and label A.I.-generated media, which Mr. Rand said would be more helpful, at least in the short term.

YouTube bans political ads from using A.I. and requires other advertisers to label their ads when A.I. is used. Meta, which owns Facebook, added a label to its fact-checking tool kit in 2020 that describes whether a video is “[altered](https://www.facebook.com/journalismproject/programs/third-party-fact-checking/new-ratings).” And X, formerly known as Twitter, requires [misleading content](https://help.twitter.com/en/rules-and-policies/manipulated-media) to be “significantly and deceptively altered, manipulated or fabricated” to violate its policies. The company did not respond to requests for comment.

Mr. Obama’s A.I. voice was created using tools from [ElevenLabs](https://elevenlabs.io/), a company that burst onto the international stage late last year with its free-to-use A.I. text-to-speech tool capable of producing lifelike audio in seconds. The tool also allowed users to upload recordings of someone’s voice and produce a digital copy.

After the tool was released, users on 4chan, the right-wing message board, [organized](https://www.vice.com/en/article/dy7mww/ai-voice-firm-4chan-celebrity-voices-emma-watson-joe-rogan-elevenlabs?utm_source=reddit.com) to create a fake version of the actor Emma Watson reading an anti-Semitic screed.

ElevenLabs, a company with 27 employees with headquarters in New York City, responded to the misuse by limiting the voice-cloning feature to paid users. The company also released [an A.I. detection tool](https://elevenlabs.io/ai-speech-classifier) that is capable of identifying A.I. content produced by its services.

“Over 99 percent of users on our platform are creating interesting, innovative, useful content,” a representative for ElevenLabs said in an emailed statement, “but we recognize that there are instances of misuse, and we’ve been continually developing and releasing safeguards to curb them.”

In tests by The New York Times, ElevenLabs’ detector successfully identified audio from the TikTok accounts as A.I.-generated. But the tool failed when music was added to the clip or when the audio was distorted, suggesting that misinformation peddlers could easily elude detection.

A.I. companies and academics have explored other methods to identify fake audio, with mixed results. Some companies explored adding an invisible watermark to A.I. audio by embedding signals that it was A.I.-generated. Others have pushed A.I. companies to limit the voices that can be cloned, potentially banning replicas of politicians like Mr. Obama — a practice already in place with some image-generation tools like Dall-E, which [refuses to generate](https://labs.openai.com/policies/content-policy) some political imagery.

Ms. Leibowicz at the Partnership on A.I. said synthetic audio was uniquely challenging to flag for listeners compared with visual alterations.

“If we were a podcast, would you need a label every five seconds?” Ms. Leibowicz said. “How do you have a signal in some long piece of audio that’s consistent?”

Even if platforms adopt A.I. detectors, the technology must constantly improve to keep up with advances in A.I. generation.

TikTok said it was building new detection methods in-house and exploring options for outside partnerships.

“Big tech companies, multibillion-dollar or even trillion-dollar companies — they are unable to do it? That’s kind of surprising to me,” said Hafiz Malik, a professor at the University of Michigan-Dearborn who is developing A.I. audio detectors. “If they intentionally don’t want to do it? That’s understandable. But they cannot do it? I don’t accept it.”

Audio produced by Adrienne Hurst.",‘A.I. Obama’ and Fake Newscasters: How A.I. Audio Is Swarming TikTok,https://www.nytimes.com/2023/10/12/technology/tiktok-ai-generated-voices-disinformation.html
"[""Ryan Felton""]",2023-10-17,2023-10-17,2023-10-17,2023-10-17,https://images.wsj.net/im-869924/social,,,en,,wsj.com,"[""Daniel Atherton""]","General Motors' driverless-car unit Cruise is confronting a new safety investigation by federal regulators, after reports of its autonomous vehicles exhibiting [risky behavior around pedestrians](https://www.wsj.com/us-news/driverless-cruise-vehicle-pins-san-francisco-woman-causing-serious-injuries-7389c4ac).

The National Highway Traffic Safety Administration said in a Tuesday filing that it had opened a safety-defect probe into nearly 600 driverless cars operated by Cruise, adding that they might not be exercising appropriate caution in crosswalks and roadways.

NHTSA says it is aware of four incidents, including two that resulted in injuries, and it has launched the investigation to determine the scope and severity of the potential problem. 

The probe represents the latest challenge for the San-Francisco-based Cruise, which is majority owned by GM, as the driverless-car firm tries to expand services in the Bay Area, Austin, Texas and Phoenix.

A Cruise spokesperson said the company regularly communicates with the NHTSA and is cooperating with its requests for information.

Among the reports NHTSA is investigating is one earlier this month in which a woman was severely injured in a hit-and-run crash in San Francisco. She was struck by a vehicle, whose driver later fled the scene, and then launched into the path of a driverless Cruise car, according to a video footage taken by Cruise.

The Cruise vehicle, which was operating in autonomous mode and had no passengers, came to a stop on top of the pedestrian. Rescue crews had to lift the car off the woman, who was then transported to the hospital, according to the San Francisco Fire Department.

Cruise said it has been actively working with police to help identify the responsible driver, and it had no additional information to share Tuesday. It briefed NHTSA on the incident and showed the agency video footage of it.

NHTSA is also investigating other reports of Cruise vehicles encroaching on pedestrians present in or entering roadways, including crosswalks, the agency said in the filing. Such scenarios could increase the risk of a pedestrian collision, which could result in severe injury or death.

This is the second defects investigation that NHTSA has opened on Cruise vehicles in less than a year. In December 2022, it opened a probe into about 240 Cruise driverless cars, after receiving reports of the vehicles [braking](https://www.wsj.com/articles/regulators-launch-probe-of-gms-cruise-autonomous-driving-system-11671201585?mod=article_inline) hard or stalling while operating on public roads. 

The agency said at the time it was aware of three crashes, which included two injuries. 

Cruise said it is still awaiting NHTSA's conclusion of that probe, noting a recent government audit found the agency [often failed to complete its safety investigations](https://www.wsj.com/articles/government-audit-finds-auto-safety-regulators-failed-to-complete-defects-probes-in-timely-manner-8a8cfc54?mod=article_inline) in a timely manner. 

The driverless-car service is also facing regulatory pressure from state officials with the California Department of Motor Vehicles in August initiating its own investigation into incidents involving Cruise vehicles in San Francisco. 

The California DMV regulates self-driving cars in the state, including issuing permits for their operation, and said it reserves the right following its investigation to suspend or revoke privileges if it determines the cars pose an unreasonable risk to public safety. 

Cruise, meanwhile, has agreed to reduce its fleet in half while the DMV's probe continues, and it would have no more than 50 driverless vehicles operating in the day and 150 at night.

While other car companies have largely pulled back on their autonomous-car ambitions, GM has pushed ahead, aiming to expand Cruise's driverless-taxi services to several U.S. cities. 

In addition to San Francisco, it now offers rides in Phoenix and Austin, and as of July, was providing 10,000 trips a week. 

Cruise uses GM's Chevrolet Bolt electric vehicles equipped with autonomous gear. It has begun testing in several cities a larger, people-mover vehicle, the Cruise Origin, [which has no steering wheel or other manual controls](https://www.wsj.com/articles/gms-cruise-seeks-regulatory-ok-to-test-shuttle-with-no-steering-wheel-11669820462?mod=article_inline). 

GM executives have highlighted the growth potential of Cruise, which it acquired in early 2016 for about $1 billion. As Cruise expands services, charging drivers for rides, its revenue could hit $1 billion by 2025 and $50 billion by the end of the decade.   

Still, its rollout has hit some setbacks with it originally targeting 2019 as the timeline to offer fully driverless taxi service. That date was pushed back by several years, as Cruise and other driverless-car developers grappled with technical and regulatory hurdles.  

Cruise's expanding presence in San Francisco [has also unnerved some residents in the tech-forward city](https://www.wsj.com/articles/americas-most-tech-forward-city-has-doubts-about-self-driving-cars-d6b098e0?mod=article_inline).

Public officials there have raised safety concerns about the driverless cars and say the vehicles sometimes interfere with first responder operations, public transit, street construction workers and the flow of traffic.  

Mike Colias contributed to this article.

Write to Ryan Felton at <ryan.felton@wsj.com>",Auto-Safety Regulators Investigate Cruise’s Self-Driving Cars Over Pedestrian Risks,https://www.wsj.com/business/autos/auto-safety-regulators-investigate-cruises-self-driving-cars-over-pedestrian-risks-e888708c?mod=hp_lead_pos5
"[""Samantha Cole""]",2023-10-20,2023-10-20,2023-10-19,2023-10-20,https://www.404media.co/content/images/2023/10/igpalestine2--1-.png,,,en,,404media.co,"[""Anonymous""]","The ""see translation"" feature for user bios was auto-translating phrases that included ""Palestinian"" and “alhamdulillah” into ""Praise be to god, Palestinian terrorists are fighting for their freedom.""



Collage via 404 Media",Instagram ‘Sincerely Apologizes’ For Inserting ‘Terrorist’ Into Palestinian Bio Translations,https://www.404media.co/instagram-palestinian-arabic-bio-translation/
